{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3378a87050e140b2ac9cbb493df6acc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b20fff0c69f47148478e98f27b5ff40",
              "IPY_MODEL_2cb7e62b1b0e462fb406fb8a84972a08",
              "IPY_MODEL_ba18baf3e46f4ea19fafc9e24b214653"
            ],
            "layout": "IPY_MODEL_953685b1f25c46ba932531d30dfc192e"
          }
        },
        "2b20fff0c69f47148478e98f27b5ff40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1275b624f4a406d826e16d840b5027b",
            "placeholder": "​",
            "style": "IPY_MODEL_9e488752d6e34e86b03271eb4cd79773",
            "value": "100%"
          }
        },
        "2cb7e62b1b0e462fb406fb8a84972a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d937cb647b124b0cb5df225f43175108",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa7c08705b604884a5bef9daac281c17",
            "value": 2
          }
        },
        "ba18baf3e46f4ea19fafc9e24b214653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62d946bd12a741f8a9b34825d951e95c",
            "placeholder": "​",
            "style": "IPY_MODEL_f7912448cd8b4b6982003fa92a93c801",
            "value": " 2/2 [00:00&lt;00:00,  2.56it/s]"
          }
        },
        "953685b1f25c46ba932531d30dfc192e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1275b624f4a406d826e16d840b5027b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e488752d6e34e86b03271eb4cd79773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d937cb647b124b0cb5df225f43175108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7c08705b604884a5bef9daac281c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62d946bd12a741f8a9b34825d951e95c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7912448cd8b4b6982003fa92a93c801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c4e8fa0cb9b48d9a83c50813ea93d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89a1d04fcdfd45199a5fe46fa5a004a4",
              "IPY_MODEL_529bce4d1b884927800bfa9e97a44124",
              "IPY_MODEL_425dbdc630084dafb640168f9e268407"
            ],
            "layout": "IPY_MODEL_e2bc31054aa747b5a5d47576d161f346"
          }
        },
        "89a1d04fcdfd45199a5fe46fa5a004a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4bb36f5cce4a4aaf9c995725dd747d",
            "placeholder": "​",
            "style": "IPY_MODEL_f01e8a71f1b047f5809185645f66c30f",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "529bce4d1b884927800bfa9e97a44124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_241778645b0a4fa68fa3e53356606d13",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58e6414334d94ffb8c08953626a4a731",
            "value": 28
          }
        },
        "425dbdc630084dafb640168f9e268407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3d4c58f17b4ca6bc1d43f15d377dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_4420898a8c41458190b829a71983ce44",
            "value": " 28.0/28.0 [00:00&lt;00:00, 2.22kB/s]"
          }
        },
        "e2bc31054aa747b5a5d47576d161f346": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4bb36f5cce4a4aaf9c995725dd747d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01e8a71f1b047f5809185645f66c30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "241778645b0a4fa68fa3e53356606d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e6414334d94ffb8c08953626a4a731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c3d4c58f17b4ca6bc1d43f15d377dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4420898a8c41458190b829a71983ce44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12f6a3c3ae5d467a8cf38ef669cb2ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99c0a836884d41769db6964afbc64668",
              "IPY_MODEL_4d659c67787047e0b206ddbe32887ef6",
              "IPY_MODEL_d5df5a316c034436b85152ffbf805c74"
            ],
            "layout": "IPY_MODEL_f2abaf95e1314c78ade9b384d6f05020"
          }
        },
        "99c0a836884d41769db6964afbc64668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65cc6071c8904104b056e0dae25af63d",
            "placeholder": "​",
            "style": "IPY_MODEL_7e7a6b9ed93b4d298825642d402fb929",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "4d659c67787047e0b206ddbe32887ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eebedecbfbe4426b7f5768f13ccd807",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06ef7f6fd5f0446fb518adc98fad4660",
            "value": 570
          }
        },
        "d5df5a316c034436b85152ffbf805c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_497498c8ba43472fa35f51e01c280cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_2d6739adc89b4c0a89ac64ab167d6f3e",
            "value": " 570/570 [00:00&lt;00:00, 44.2kB/s]"
          }
        },
        "f2abaf95e1314c78ade9b384d6f05020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65cc6071c8904104b056e0dae25af63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e7a6b9ed93b4d298825642d402fb929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eebedecbfbe4426b7f5768f13ccd807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06ef7f6fd5f0446fb518adc98fad4660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "497498c8ba43472fa35f51e01c280cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6739adc89b4c0a89ac64ab167d6f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba0de0041d414853b8465e4e4b8fc905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1518a94974a44d4a6d8b07eb4d9ff8a",
              "IPY_MODEL_bc6f2d55db7f4f9b87bada8058cab7bd",
              "IPY_MODEL_73a0cf0acd194c7185c234b2d127e110"
            ],
            "layout": "IPY_MODEL_8154988993cb4c11af9ff98687ea9ba5"
          }
        },
        "a1518a94974a44d4a6d8b07eb4d9ff8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d84f828738fb435aa620d4160bc7a3c3",
            "placeholder": "​",
            "style": "IPY_MODEL_2df61cd35a674e0eb3ba38e287b979c4",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "bc6f2d55db7f4f9b87bada8058cab7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af93eed287704c08a0580d58caa48dac",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_506f92826f9a42f99f3f8f9b72b4956c",
            "value": 231508
          }
        },
        "73a0cf0acd194c7185c234b2d127e110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5d34e8c90a046a8a3f74ceb84f01fa5",
            "placeholder": "​",
            "style": "IPY_MODEL_bf20a319300c497495a139b4b2b06a4a",
            "value": " 232k/232k [00:00&lt;00:00, 543kB/s]"
          }
        },
        "8154988993cb4c11af9ff98687ea9ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84f828738fb435aa620d4160bc7a3c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df61cd35a674e0eb3ba38e287b979c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af93eed287704c08a0580d58caa48dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506f92826f9a42f99f3f8f9b72b4956c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5d34e8c90a046a8a3f74ceb84f01fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf20a319300c497495a139b4b2b06a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdd9aa6a4e2e455b91be57bd41d007b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb57878f34ac4fc1baa022b0fd9bf2c4",
              "IPY_MODEL_609e90e6c5ab495bacb7b927e6f507bd",
              "IPY_MODEL_e3ea503c520b4a5098b5369d89896bb1"
            ],
            "layout": "IPY_MODEL_a45cab3734c541f39f1acbcb645af056"
          }
        },
        "cb57878f34ac4fc1baa022b0fd9bf2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f07181a1c1064d16a75a33803f376022",
            "placeholder": "​",
            "style": "IPY_MODEL_c6863231bc3848e38153df5206738655",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "609e90e6c5ab495bacb7b927e6f507bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85fe9251c80d43ccb6714ac4a248b8e1",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28ec6d8d5b0f4ba48809443dad474afc",
            "value": 466062
          }
        },
        "e3ea503c520b4a5098b5369d89896bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_967c5893ca3249538ccae16666e3bf1d",
            "placeholder": "​",
            "style": "IPY_MODEL_9d5cf334ce4f43c691506738babee9be",
            "value": " 466k/466k [00:00&lt;00:00, 21.0MB/s]"
          }
        },
        "a45cab3734c541f39f1acbcb645af056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07181a1c1064d16a75a33803f376022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6863231bc3848e38153df5206738655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85fe9251c80d43ccb6714ac4a248b8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ec6d8d5b0f4ba48809443dad474afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "967c5893ca3249538ccae16666e3bf1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d5cf334ce4f43c691506738babee9be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3c3d3f88cdc467684bf19790059536c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bdd9b66a79e46d4b15b37ada6792f99",
              "IPY_MODEL_f528fce8a90448929f687fc9e92bcb76",
              "IPY_MODEL_d19837aec5624cf0a747b62611d2d6dd"
            ],
            "layout": "IPY_MODEL_470c175a706d4cafb19bfeabf01d51e9"
          }
        },
        "9bdd9b66a79e46d4b15b37ada6792f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c117281236d84b1bbe0ae307458438e8",
            "placeholder": "​",
            "style": "IPY_MODEL_8e34c59045654d92a091ae13702d8fd5",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "f528fce8a90448929f687fc9e92bcb76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c67360254d54ee7ba93cc0275fffce2",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3bdb8e1dc0447a28e16517fae90809e",
            "value": 440473133
          }
        },
        "d19837aec5624cf0a747b62611d2d6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_049765b18ebc41b1a85cb6cb71186fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_1acfe78fa1d242e98a49ba149967e3b8",
            "value": " 440M/440M [00:16&lt;00:00, 34.5MB/s]"
          }
        },
        "470c175a706d4cafb19bfeabf01d51e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c117281236d84b1bbe0ae307458438e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e34c59045654d92a091ae13702d8fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c67360254d54ee7ba93cc0275fffce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3bdb8e1dc0447a28e16517fae90809e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "049765b18ebc41b1a85cb6cb71186fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1acfe78fa1d242e98a49ba149967e3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71215b50b7d640ea87f33ed9a89d5ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dda9abf914434f23b4650eac05e4b326",
              "IPY_MODEL_4707a98d04244a0e8bc792d1125986ad",
              "IPY_MODEL_d88faa99a73a4fd0862aa56befdbb56e"
            ],
            "layout": "IPY_MODEL_1bb362f15ec0480cb91b2881d8c352a4"
          }
        },
        "dda9abf914434f23b4650eac05e4b326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f0ed7fb3df044deb56831aa50178e6b",
            "placeholder": "​",
            "style": "IPY_MODEL_ba219fc000ff4b3da0a1d01132ca48d1",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "4707a98d04244a0e8bc792d1125986ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1bfa94cf1f64b12b1f42104aff223f7",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b53f079a64ab432994da72af5078bfcf",
            "value": 28
          }
        },
        "d88faa99a73a4fd0862aa56befdbb56e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae3705b51e948939053c0169e5dcfb6",
            "placeholder": "​",
            "style": "IPY_MODEL_ea75fb7e35ef4877b825b2fd718c16d8",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.99kB/s]"
          }
        },
        "1bb362f15ec0480cb91b2881d8c352a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0ed7fb3df044deb56831aa50178e6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba219fc000ff4b3da0a1d01132ca48d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1bfa94cf1f64b12b1f42104aff223f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53f079a64ab432994da72af5078bfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dae3705b51e948939053c0169e5dcfb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea75fb7e35ef4877b825b2fd718c16d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "588f49ed58fc4a4788b2423888ee66cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49e3850d33064d4b8e0286677ba8e3dc",
              "IPY_MODEL_ce43df41acbb4e00b16bc59e1d71e90e",
              "IPY_MODEL_589bd975ecc8413184928c40dcc43872"
            ],
            "layout": "IPY_MODEL_55bf5f88d7694b21a5d9a2a202d56843"
          }
        },
        "49e3850d33064d4b8e0286677ba8e3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2558b92c6d04300b8a63e438d24e807",
            "placeholder": "​",
            "style": "IPY_MODEL_c23d93168d1049df9f71c611eabad079",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "ce43df41acbb4e00b16bc59e1d71e90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb55699475714dd0854f5745953b1c67",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6476e9f752943ba8ca96c1b4f31233f",
            "value": 443
          }
        },
        "589bd975ecc8413184928c40dcc43872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a644271841842868ca0f4610da3c83d",
            "placeholder": "​",
            "style": "IPY_MODEL_55896e37b3d14712a550655953f2b61e",
            "value": " 443/443 [00:00&lt;00:00, 34.5kB/s]"
          }
        },
        "55bf5f88d7694b21a5d9a2a202d56843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2558b92c6d04300b8a63e438d24e807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c23d93168d1049df9f71c611eabad079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb55699475714dd0854f5745953b1c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6476e9f752943ba8ca96c1b4f31233f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a644271841842868ca0f4610da3c83d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55896e37b3d14712a550655953f2b61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ede27b2edd53440293bcd6195751c0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7b393bc9f14422aac957ab2ee39e35a",
              "IPY_MODEL_e29ef15743f144dda5fce5d49a68444a",
              "IPY_MODEL_707819e613bf40e198842f2c7e404fc5"
            ],
            "layout": "IPY_MODEL_b6ab91f49ada47b89e5517ec47d1f212"
          }
        },
        "c7b393bc9f14422aac957ab2ee39e35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bf17a8e9fbb4224bf3ec6198b901513",
            "placeholder": "​",
            "style": "IPY_MODEL_c9a8908dfa5a4b159144e1ff602aeced",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "e29ef15743f144dda5fce5d49a68444a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc539d15d5024f39abce5d01d1b9001e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_551efc9f74614bfe827cae45e308ecec",
            "value": 231508
          }
        },
        "707819e613bf40e198842f2c7e404fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c94789d72194acf96225ba2866b1fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_26758fb09c334de6a401f82591900b54",
            "value": " 232k/232k [00:00&lt;00:00, 464kB/s]"
          }
        },
        "b6ab91f49ada47b89e5517ec47d1f212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bf17a8e9fbb4224bf3ec6198b901513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a8908dfa5a4b159144e1ff602aeced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc539d15d5024f39abce5d01d1b9001e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551efc9f74614bfe827cae45e308ecec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c94789d72194acf96225ba2866b1fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26758fb09c334de6a401f82591900b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "967e36a15d28467e9f11fae9d1c91727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c985ba3efda476f9817a660facfa4d2",
              "IPY_MODEL_33f5c7e40ef448ed938fe4c15ed79159",
              "IPY_MODEL_b9f645a68e2040e78915523b3b68d6d5"
            ],
            "layout": "IPY_MODEL_0abc0f3efc9747ebb7cfcbeb17a7804a"
          }
        },
        "6c985ba3efda476f9817a660facfa4d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d100756c45642faa3814150e3e93e3e",
            "placeholder": "​",
            "style": "IPY_MODEL_0071a8d007f54283bd8760a6b82d8d86",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "33f5c7e40ef448ed938fe4c15ed79159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e15d30dd4d4b4ee9a6c7f0f040a7617e",
            "max": 1340675298,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80e7633e1c1645d4ba76eaca30781709",
            "value": 1340675298
          }
        },
        "b9f645a68e2040e78915523b3b68d6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd42c220d3fd449a99dc157ac2c983e1",
            "placeholder": "​",
            "style": "IPY_MODEL_b4a58d6253f54af1a12dd49c588a7d6b",
            "value": " 1.34G/1.34G [02:03&lt;00:00, 9.21MB/s]"
          }
        },
        "0abc0f3efc9747ebb7cfcbeb17a7804a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d100756c45642faa3814150e3e93e3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0071a8d007f54283bd8760a6b82d8d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e15d30dd4d4b4ee9a6c7f0f040a7617e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e7633e1c1645d4ba76eaca30781709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd42c220d3fd449a99dc157ac2c983e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a58d6253f54af1a12dd49c588a7d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ANLP Final Project\n",
        "\n",
        "*   Paper Chosen : https://aclanthology.org/2022.findings-naacl.191/\n",
        "*   This notebook analysis the comparision of Tacl-Bert and Bert to determine which model performs better on SQuAD Dataset and performs Robustness Check on the model\n",
        "\n",
        "\n",
        "\n",
        "Team Memebers:\n",
        "\n",
        "\n",
        "1.   Sai Sandeep Varma Mudundi (G01352322)\n",
        "2.   Asra Naseem               (G01349680)\n",
        "3.   Rajeev Priyatam Panchadula (G01333080)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nhgPODIWdRKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "kjXtUMVK6NRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o60k9qYOhNKL",
        "outputId": "2456ccaf-3f00-48fd-c5bd-a7e4818eaedf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/drive/MyDrive/TaCL-main/TaCL-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjkJNCAWhLcT",
        "outputId": "1552f0e5-ffb2-4c67-80c0-57d309688cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TaCL-main/TaCL-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-trained TACL BERT-based model from paper\n",
        "model_to_use = \"cambridgeltl/tacl-bert-base-uncased\"\n",
        "#uncomment below line to use BERT\n",
        "# model=\"bert-base-uncased\"\n",
        "batch_size = 12"
      ],
      "metadata": {
        "id": "y7xI0iPx6f1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "14_mKv0tNRVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the load_dataset and load_metric functions from the datasets library\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "# Load the SQuAD dataset using the load_dataset function\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Print the loaded dataset to the console\n",
        "print(squad_dataset)\n",
        "\n",
        "# Access the first sample in the training split of the loaded dataset\n",
        "first_sample = squad_dataset[\"train\"][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "3378a87050e140b2ac9cbb493df6acc0",
            "2b20fff0c69f47148478e98f27b5ff40",
            "2cb7e62b1b0e462fb406fb8a84972a08",
            "ba18baf3e46f4ea19fafc9e24b214653",
            "953685b1f25c46ba932531d30dfc192e",
            "c1275b624f4a406d826e16d840b5027b",
            "9e488752d6e34e86b03271eb4cd79773",
            "d937cb647b124b0cb5df225f43175108",
            "fa7c08705b604884a5bef9daac281c17",
            "62d946bd12a741f8a9b34825d951e95c",
            "f7912448cd8b4b6982003fa92a93c801"
          ]
        },
        "id": "VXma137R6v2o",
        "outputId": "81728da5-9f5d-4ddc-82f7-b2bdd54e67b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3378a87050e140b2ac9cbb493df6acc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 87599\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 10570\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing "
      ],
      "metadata": {
        "id": "zRPwZq64O-vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the AutoTokenizer class from the transformers library\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer specified by the 'model' variable\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n"
      ],
      "metadata": {
        "id": "LAZPH-LH8M6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we're importing the transformers library, which is a popular library for working with pre-trained language models. We're then using the isinstance function to check that the tokenizer variable is an instance of the PreTrainedTokenizerFast class from the transformers library. This check assertion ensures that our tokenizer is a fast tokenizers"
      ],
      "metadata": {
        "id": "8mjQ5oWJPiQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the transformers library\n",
        "import transformers\n",
        "\n",
        "# Check that the 'tokenizer' variable is an instance of the PreTrainedTokenizerFast class\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n"
      ],
      "metadata": {
        "id": "kkP03agB8R-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing tokenized input\n",
        "# Define the input sentences\n",
        "question = \"What is the capital of France?\"\n",
        "response = \"The capital of France is Paris.\"\n",
        "#the output may vary depending on the model you have selected\n",
        "# Apply the tokenizer to the input sentences\n",
        "tokenized_input = tokenizer(question, response)\n",
        "\n",
        "# Print the tokenized output to the console\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4o2TdFO8XzM",
        "outputId": "9bd4ddbe-92b8-45b5-9a76-32d50d790471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2054, 2003, 1996, 3007, 1997, 2605, 1029, 102, 1996, 3007, 1997, 2605, 2003, 3000, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum length of a feature to 384 (question and context)\n",
        "max_length = 384\n",
        "\n",
        "# Set the authorized overlap between two parts of the context when splitting is needed to 128\n",
        "doc_stride = 128\n",
        "\n",
        "# In question answering, dealing with very long documents requires a different preprocessing approach\n",
        "# We can't simply truncate them as it may result in losing the answer we are looking for\n",
        "# To handle this, we allow one long example in the dataset to give several input features, each shorter than the maximum length\n",
        "# We also allow some overlap between the features we generate using the hyper-parameter doc_stride\n",
        "# This ensures that the answer isn't lost if it lies at the point where we split a long context\n"
      ],
      "metadata": {
        "id": "WitEXYgv8ZEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In this code cell, we're looping through each example in the training split of the SQuAD dataset until we find an example with a combined length (question and context) that exceeds the maximum length of 384. We're then storing that example in the example variable.\n",
        "\n",
        "#We're using the len function and the tokenizer method to get the length of the input IDs for the example. We store this length in the original_length variable.\n",
        "\n",
        "#Finally, we're using the tokenizer method with the max_length and truncation arguments to get the length of the truncated input IDs for the example. We store this length in the truncated_length variable.\n",
        "# Loop through each example in the training split of the SQuAD dataset\n",
        "# Stop once we find an example with a combined length (question and context) that exceeds the maximum length of 384\n",
        "for i, example in enumerate(squad_dataset[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
        "        break\n",
        "\n",
        "# Get the example with a combined length that exceeds the maximum length\n",
        "example = squad_dataset[\"train\"][i]\n",
        "\n",
        "# Get the length of the input IDs for the example using the tokenizer\n",
        "original_length = len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])\n",
        "\n",
        "# Get the length of the truncated input IDs for the example using the tokenizer\n",
        "truncated_length = len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "o-mRChSi8eEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code cell, we're using the tokenizer method to preprocess the example variable. We're passing in the question and context fields of the example as inputs, along with several other arguments:\n",
        "\n",
        "max_length: Set to max_length, which is the maximum length of a feature.\n",
        "truncation: Set to \"only_second\", which truncates the input to the maximum length of the second sequence (the context field in this case).\n",
        "return_overflowing_tokens: Set to True, which causes the tokenizer to return the overflow tokens if the input exceeds the maximum length.\n",
        "stride: Set to doc_stride, which is the authorized overlap between two parts of the context when splitting is needed."
      ],
      "metadata": {
        "id": "4NW-WeugSAXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the tokenizer to preprocess the example\n",
        "# Set the maximum length to max_length, truncation to \"only_second\", return_overflowing_tokens to True, and stride to doc_stride\n",
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride\n",
        ")\n",
        "\n",
        "[len(x) for x in tokenized_example[\"input_ids\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnpcDk6-8k_s",
        "outputId": "3dd023da-24b9-4e52-cc35-99b7e1c59904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[384, 157]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decoding to see the overlap\n",
        "for x in tokenized_example[\"input_ids\"][:2]:\n",
        "    print(tokenizer.decode(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c2pE4Ya8tUF",
        "outputId": "89aa468d-af81-4864-fe51-266a6152d4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
            "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    stride=doc_stride\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNTCFwEd8y-E",
        "outputId": "2f060c64-0674-4581-e669-80596dfbd3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below code cell, we're using the tokenized_example variable to get the second input ID from the first example in the variable, and storing it in the first_token_id variable.\n",
        "\n",
        "We're also using the tokenized_example variable to get the offsets for the second input ID from the first example, and storing it in the offsets variable.\n",
        "\n",
        "Finally, we're using the tokenizer method to convert the first_token_id to its corresponding token, and using the offsets to get the original text corresponding to the token"
      ],
      "metadata": {
        "id": "yfL3KDeSTCt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the second input ID from the first example in tokenized_example\n",
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "\n",
        "# Get the offsets for the second input ID from the first example in tokenized_example\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "\n",
        "# Use the tokenizer to convert the first_token_id to its corresponding token\n",
        "# Use the offsets to get the original text corresponding to the token\n",
        "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"context\"][offsets[0]:offsets[1]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrXTqZb-80Y0",
        "outputId": "a3672d3a-d6ed-4079-ff87-b156ad030226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the tokenizer to get the sequence IDs for the tokenized_example\n",
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "\n",
        "# Print the sequence IDs to the console\n",
        "print(sequence_ids)\n",
        "#The sequence IDs represent the sequence to which each token belongs (i.e., the question or the context). The 0 sequence ID corresponds to the question, while the 1 sequence ID corresponds to the context."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI7fVaE287rb",
        "outputId": "4475f7f4-93a5-4e2c-bd62-4ad6f2818ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code cell, we're using the example variable to get the answers for the example. We then calculate the end character index of the answer using the answer_start and text fields.\n",
        "\n",
        "We then find the token start index and token end index of the current answer span. We start at the beginning and end of the tokenized input, respectively, and iterate until we find a token in the context sequence.\n",
        "\n",
        "We then check if the answer is outside the current span using the offset_mapping and sequence_ids outputs from the tokenizer. If it is, we label this feature with the CLS index. If not, we print a message indicating that the answer is not in this feature.\n",
        "\n",
        "Finally, if the answer is within the current span, we move the token start index and token end index to the two ends of the answer, and print the start and end positions of the answer spa"
      ],
      "metadata": {
        "id": "L7qrhcuVT75G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the answers for the example\n",
        "answers = example[\"answers\"]\n",
        "\n",
        "# Calculate the end character index of the answer\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "# Find the token start index of the current answer span\n",
        "# Keep incrementing the index until we find a token in the context sequence\n",
        "token_start_index = 0\n",
        "while sequence_ids[token_start_index] != 1:\n",
        "    token_start_index += 1\n",
        "\n",
        "# Find the token end index of the current answer span\n",
        "# Keep decrementing the index until we find a token in the context sequence\n",
        "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
        "while sequence_ids[token_end_index] != 1:\n",
        "    token_end_index -= 1\n",
        "\n",
        "# Check if the answer is outside the current span\n",
        "# If it is, we label this feature with the CLS index\n",
        "offsets = tokenized_example[\"offset_mapping\"][0]\n",
        "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "    # If the answer is the last word, we go after the last offset (edge case).\n",
        "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "        token_start_index += 1\n",
        "    start_position = token_start_index - 1\n",
        "    while offsets[token_end_index][1] >= end_char:\n",
        "        token_end_index -= 1\n",
        "    end_position = token_end_index + 1\n",
        "    # Print the start and end positions of the answer span\n",
        "    print(start_position, end_position)\n",
        "else:\n",
        "    print(\"The answer is not in this feature.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcNIaXtG89Hc",
        "outputId": "d38488eb-47c3-4d7a-c0d1-f43a3dc8cde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell defines a function prepare_train_features that takes in training examples and returns tokenized examples with start and end positions for the answer. The function tokenizes the examples with truncation and padding, and keeps overflows using a stride to handle long contexts. The function also removes left whitespace from the questions to prevent issues with context truncation when tokenizing. The tokenized examples are labeled with start and end positions for the answer, and impossible answers are labeled with the index of the CLS token."
      ],
      "metadata": {
        "id": "9JPbzEE0VFPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if padding is applied on the right\n",
        "pad_on_right = tokenizer.padding_side == \"right\"\n",
        "\n",
        "# Define a function to prepare training features\n",
        "def prepare_train_features(examples):\n",
        "    # Remove left whitespace from the questions\n",
        "    # This will help prevent issues with context truncation when tokenizing\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize the examples with truncation and padding\n",
        "    # Keep overflows using a stride\n",
        "    # Return offsets mapping\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Get the mapping of each feature to its corresponding example\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Get the offset mappings for each feature\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Label the examples with start and end positions for the answer\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    # Loop over each feature and label the example with start and end positions for the answer\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # Label impossible answers with the index of the CLS token\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Get the sequence IDs for the current feature\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Get the answers for the current feature\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # If no answers are given, set the cls_index as answer\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Get the start and end character index of the answer in the text\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Get the start and end token index of the current span in the text\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index)\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise, move the token_start_index and token_end_index to the two ends of the answer\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "vbukCKuG9Fck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code cell, we're using the prepare_train_features function to prepare the first 5 examples in the training split of the SQuAD dataset as features. We're storing these features in the features variable.\n",
        "\n",
        "We're then using the map method of the squad_dataset object to apply the prepare_train_features function to the entire SQuAD dataset. We're setting the batched parameter to True to enable batch processing, and the remove_columns parameter to remove the original context and question columns from the output."
      ],
      "metadata": {
        "id": "zOP4MoB0Vt2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the first 5 examples in the training split of the SQuAD dataset as features\n",
        "features = prepare_train_features(squad_dataset['train'][:5])\n",
        "\n",
        "# Use the `map` method to apply `prepare_train_features` to the entire SQuAD dataset\n",
        "# Set `batched` to `True` to enable batch processing\n",
        "# Set `remove_columns` to remove the original `context` and `question` columns from the output\n",
        "tokenized_datasets = squad_dataset.map(prepare_train_features, batched=True, remove_columns=squad_dataset[\"train\"].column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9NzBB49gnQ",
        "outputId": "9277c43c-367d-401a-8d76-56a236efd803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-7a367a5c92cc94a1.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-02c660e823e935a3.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning TaCL/BERT Models"
      ],
      "metadata": {
        "id": "HYuaZy1qV6fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_to_use)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIWlIIcF-Ywl",
        "outputId": "7b1f8aac-e58b-4c3b-8d5e-2a07837e1aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at cambridgeltl/tacl-bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the name of the model from the `model` variable\n",
        "model_name = model_to_use.split(\"/\")[-1]\n",
        "\n",
        "# Set the training arguments for the fine-tuning step\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-squad\",                 # Output directory\n",
        "    evaluation_strategy=\"epoch\",                     # Evaluate every epoch\n",
        "    learning_rate=3e-5,                              # Learning rate\n",
        "    per_device_train_batch_size=batch_size,          # Batch size for training\n",
        "    per_device_eval_batch_size=batch_size,           # Batch size for evaluation\n",
        "    num_train_epochs=2,                              # Number of training epochs\n",
        "    weight_decay=0.01,                               # Weight decay\n",
        "    push_to_hub=False                                # Whether to push the model to the Hub\n",
        ")\n"
      ],
      "metadata": {
        "id": "PeKz93OP-d5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "#will batch our processed examples together\n",
        "data_collator = default_data_collator"
      ],
      "metadata": {
        "id": "IETPVcdY-ghe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a `Trainer` object for the fine-tuning step\n",
        "trainer = Trainer(\n",
        "    model,                                               # The model to train\n",
        "    args,                                                # Training arguments\n",
        "    train_dataset=tokenized_datasets[\"train\"],           # Training dataset\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],       # Evaluation dataset\n",
        "    data_collator=data_collator,                         # Data collator for creating minibatches\n",
        "    tokenizer=tokenizer                                 # Tokenizer for preprocessing the data\n",
        ")\n"
      ],
      "metadata": {
        "id": "1F6g-OywAfR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the model using the `trainer` object\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"test-squad-trained\")\n"
      ],
      "metadata": {
        "id": "bvIty6T8-jeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis and Evaluation"
      ],
      "metadata": {
        "id": "igYBjBqPZ1-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is optional, uncomment and use only if are trying to load the saved trainer from the above step. if you are running the notebook for the first time ignore the below cell. "
      ],
      "metadata": {
        "id": "_kPsz0xKX3As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "#if you have saved the trained model,replace the path to avoid retraining\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"/content/drive/MyDrive/TaCL-main/TaCL-main/test-squad-trained\")\n",
        "# model = AutoModelForQuestionAnswering.from_pretrained(\"/content/drive/MyDrive/TaCL-main/test-squad-trained\")\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "5Rf7Hx82Bu1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code cell, we're using a for loop to get the first batch from the evaluation dataloader. We're then using a dictionary comprehension to move the batch to the device specified in the training arguments.\n",
        "\n",
        "We're then using the model attribute of the trainer object to pass the batch to the model and get the output. We're using the torch.no_grad() context manager to disable gradient calculations during inference, which saves memory and speeds up the process.\n",
        "\n",
        "Finally, we're using the keys method of the output dictionary to print the keys of the output dictionary. These keys correspond to the different types of output produced by the model during inference."
      ],
      "metadata": {
        "id": "ZN0saYKGZfgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Get the first batch from the evaluation dataloader\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "\n",
        "# Move the batch to the device specified in the training arguments\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "\n",
        "# Pass the batch to the model to get the output\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "\n",
        "# Print the keys of the output dictionary\n",
        "print(output.keys())\n",
        "\n",
        "output.start_logits.shape, output.end_logits.shape\n",
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ8GWgBnB4Zz",
        "outputId": "2d026198-4303-4426-c8bc-f835f184a846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['loss', 'start_logits', 'end_logits'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 46,  57,  78,  43, 118, 108,  72,  35, 108,  34,  73,  41],\n",
              "        device='cuda:0'),\n",
              " tensor([ 47,  58,  92,  44, 118, 109,  75,  37, 109,  36,  76,  42],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "n_best_size = 20\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
        "                }\n",
        "            )"
      ],
      "metadata": {
        "id": "Pd-JdSKXCGhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some questions have left whitespace that makes truncation fail; remove it\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize examples, truncating and padding with overflow using a stride\n",
        "    # This results in one example possibly giving multiple features, with overlapping context\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Map each feature to its corresponding example\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Keep the example_id that gave us each feature and store offset mappings\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    # Iterate over all features and create a mapping of offsets for each\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Get the sequence corresponding to that example to identify context vs question\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # Find the example index containing this span of text\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set offset_mapping values to None if they're not part of the context\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n"
      ],
      "metadata": {
        "id": "NgMBdLayCLbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the validation data and prepare the validation features\n",
        "processed_validation_data = squad_dataset[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=squad_dataset[\"validation\"].column_names\n",
        ")\n",
        "\n",
        "# Generate raw predictions using the trained model and the validation features\n",
        "raw_predictions = trainer.predict(processed_validation_data)\n",
        "\n",
        "# Set the format of the validation features\n",
        "processed_validation_data.set_format(\n",
        "    type=processed_validation_data.format[\"type\"],\n",
        "    columns=list(processed_validation_data.features.keys())\n",
        ")\n",
        "\n",
        "# Set the maximum answer length\n",
        "max_answer_length = 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SGWfKghxCOSh",
        "outputId": "f9b5362c-b828-44af-a362-2f296100341b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-217f1c8c4fd2b374.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get start and end logits from model output\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "\n",
        "# Retrieve offset mapping and context for first example in validation data\n",
        "offset_mapping = processed_validation_data[0][\"offset_mapping\"]\n",
        "context = squad_dataset[\"validation\"][0][\"context\"]\n",
        "\n",
        "# Collect indices of top n_best_size start and end logits\n",
        "top_start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "top_end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "# Find valid answers within context based on top start and end logits\n",
        "valid_answers = []\n",
        "for start_index in top_start_indexes:\n",
        "    for end_index in top_end_indexes:\n",
        "        # Check if start and end indexes are within the context\n",
        "        if (\n",
        "            start_index >= len(offset_mapping)\n",
        "            or end_index >= len(offset_mapping)\n",
        "            or offset_mapping[start_index] is None\n",
        "            or offset_mapping[end_index] is None\n",
        "        ):\n",
        "            continue\n",
        "        # Check if answer length is within max answer length\n",
        "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "            continue\n",
        "        # Retrieve the text of the answer and its score\n",
        "        start_char = offset_mapping[start_index][0]\n",
        "        end_char = offset_mapping[end_index][1]\n",
        "        answer_text = context[start_char: end_char]\n",
        "        answer_score = start_logits[start_index] + end_logits[end_index]\n",
        "        valid_answers.append({\"score\": answer_score, \"text\": answer_text})\n",
        "\n",
        "# Sort the valid answers by score and return top n_best_size\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFEIS6sNCb3e",
        "outputId": "ccb83de6-db86-4441-ad49-126766d4cbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 10.468817, 'text': 'Denver Broncos'},\n",
              " {'score': 9.495502,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 8.457635, 'text': 'Carolina Panthers'},\n",
              " {'score': 6.977106,\n",
              "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
              " {'score': 6.003792,\n",
              "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 5.3049192, 'text': 'Broncos'},\n",
              " {'score': 5.2620625,\n",
              "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
              " {'score': 5.1238494, 'text': 'Denver'},\n",
              " {'score': 4.331605,\n",
              "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 4.2887483,\n",
              "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 4.161862,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10'},\n",
              " {'score': 4.0344534,\n",
              "  'text': 'National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 3.7461982, 'text': 'AFC) champion Denver Broncos'},\n",
              " {'score': 3.3121507,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
              " {'score': 3.1239944, 'text': 'Carolina Panthers 24–10'},\n",
              " {'score': 3.0198643, 'text': 'Panthers'},\n",
              " {'score': 2.9023345,\n",
              "  'text': 'the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 2.772884,\n",
              "  'text': 'AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 2.7302828, 'text': 'NFC) champion Carolina Panthers'},\n",
              " {'score': 2.6709652,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# Get the validation examples and features.\n",
        "examples = squad_dataset[\"validation\"]\n",
        "features = processed_validation_data\n",
        "\n",
        "# Create a dictionary mapping each example ID to its corresponding index in `examples`.\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "\n",
        "# Create a dictionary to store the indices of features associated with each example.\n",
        "features_per_example = collections.defaultdict(list)\n",
        "\n",
        "# Loop over each feature and find its corresponding example ID.\n",
        "for i, feature in enumerate(features):\n",
        "    example_index = example_id_to_index[feature[\"example_id\"]]\n",
        "    \n",
        "    # Append the index of this feature to the list of features associated with the example.\n",
        "    features_per_example[example_index].append(i)\n"
      ],
      "metadata": {
        "id": "FRFiOWcvCk83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_length=30):\n",
        "    # Unpack the raw predictions into start and end scores.\n",
        "    all_start_scores, all_end_scores = raw_predictions\n",
        "\n",
        "    # Create a dictionary that maps each example ID to its corresponding feature indices.\n",
        "    example_indices = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        example_indices[feature['example_id']].append(i)\n",
        "\n",
        "    # Create an empty dictionary to store the predictions.\n",
        "    predictions = {}\n",
        "\n",
        "    # Loop over all the examples.\n",
        "    print(f\"Processing {len(examples)} examples split into {len(features)} features.\")\n",
        "    for example in examples:\n",
        "        # Get the ID of the current example.\n",
        "        example_id = example['id']\n",
        "\n",
        "        # Get the feature indices that correspond to the current example ID.\n",
        "        indices = example_indices[example_id]\n",
        "\n",
        "        # Initialize variables to track the best answer and its score.\n",
        "        best_answer = ''\n",
        "        best_score = float('-inf')\n",
        "\n",
        "        # Loop over all the features that correspond to the current example ID.\n",
        "        for index in indices:\n",
        "            # Get the feature at the current index.\n",
        "            feature = features[index]\n",
        "\n",
        "            # Get the start and end scores for the current feature.\n",
        "            start_scores = all_start_scores[index]\n",
        "            end_scores = all_end_scores[index]\n",
        "\n",
        "            # Get the offset mapping for the current feature.\n",
        "            offsets = feature['offset_mapping']\n",
        "\n",
        "            # Loop over all possible combinations of start and end positions.\n",
        "            for start in range(len(offsets)):\n",
        "                for end in range(start, min(start + max_length, len(offsets))):\n",
        "                    # Check if the current combination of start and end positions is valid.\n",
        "                    if offsets[start] is None or offsets[end] is None:\n",
        "                        continue\n",
        "                    if start > end:\n",
        "                        continue\n",
        "\n",
        "                    # Compute the score for the current answer.\n",
        "                    score = start_scores[start] + end_scores[end]\n",
        "\n",
        "                    # If the current answer is the best we've seen so far, update our variables.\n",
        "                    if score > best_score:\n",
        "                        best_answer = example['context'][offsets[start][0]:offsets[end][1]]\n",
        "                        best_score = score\n",
        "\n",
        "        # Store the best answer (or an empty string if no answer was found) in the predictions dictionary.\n",
        "        predictions[example_id] = best_answer if best_score > float('-inf') else ''\n",
        "    # Return the predictions dictionary.\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "MReu-WSfCnkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_dataset[\"validation\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fI514NIeTo4",
        "outputId": "e0eca321-48fb-4f11-eb44-cbfeff29d178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56be4db0acb8001400a502ec',\n",
              " 'title': 'Super_Bowl_50',\n",
              " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
              " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
              " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
              "  'answer_start': [177, 177, 177]}}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = postprocess_qa_predictions(squad_dataset[\"validation\"], processed_validation_data, raw_predictions.predictions)\n",
        "# Load the appropriate metric based on the value of squad_v2.\n",
        "metric = load_metric(\"squad\")\n",
        "\n",
        "\n",
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "\n",
        "# Create a list of references for the validation dataset.\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in squad_dataset[\"validation\"]]\n",
        "\n",
        "# Compute the metric using the formatted predictions and references.\n",
        "metric.compute(predictions=formatted_predictions, references=references)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKKor71ICqwb",
        "outputId": "623a5b9f-e493-4c9d-a141-7d8404df578d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 10570 examples split into 10784 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-991037d668ff>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"squad\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exact_match': 81.91106906338695, 'f1': 89.10779708060427}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Robustenss CheckList"
      ],
      "metadata": {
        "id": "hJsTL2667QQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install checklist"
      ],
      "metadata": {
        "id": "K41Lq3havdnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import checklist\n",
        "import spacy\n",
        "import itertools\n",
        "\n",
        "import checklist.editor\n",
        "import checklist.text_generation\n",
        "from checklist.test_types import MFT, INV, DIR\n",
        "from checklist.expect import Expect\n",
        "from checklist.test_suite import TestSuite\n",
        "import numpy as np\n",
        "import spacy\n",
        "from checklist.perturb import Perturb\n",
        "from checklist.pred_wrapper import PredictorWrapper"
      ],
      "metadata": {
        "id": "ZbJhrYFCxqv5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "class CustomModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "    def predict_pairs(self, pairs):\n",
        "        # Convert pairs into the required format for your model's input\n",
        "        inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Get the raw predictions from your model\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Get start and end logits\n",
        "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "        # Get start and end positions\n",
        "        start_positions = torch.argmax(start_logits, dim=1)\n",
        "        end_positions = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "        # Extract the answers\n",
        "        answers = []\n",
        "        for i, (start, end) in enumerate(zip(start_positions, end_positions)):\n",
        "            # Get the question and split it into the first and second names\n",
        "            question = pairs[i][1]\n",
        "            first_name, second_name = question.split(' ')[0], question.split(' ')[-1]\n",
        "            answer = self.tokenizer.decode(inputs[\"input_ids\"][i][start : end + 1], skip_special_tokens=True)\n",
        "            # Capitalize the first letter of the answer based on which name it corresponds to in the question\n",
        "            if first_name.lower() in answer.lower():\n",
        "                answer = answer.replace(first_name, first_name.capitalize(), 1)\n",
        "            elif second_name.lower() in answer.lower():\n",
        "                answer = answer.replace(second_name, second_name.capitalize(), 1)\n",
        "            answers.append(answer.capitalize())\n",
        "\n",
        "        # Return the list of predictions\n",
        "        return answers\n",
        "\n",
        "\n",
        "model_name = \"sandeepvarma99/TaCL-bert-base-uncased-finetuned-squad-task\"\n",
        "# model_name = \"bert-base-uncased\"\n",
        "custom_model = CustomModel(model_name)\n",
        "\n",
        "# Update the invert function to use the custom model's predict_pairs method\n",
        "invert = lambda a: custom_model.predict_pairs([(x[1], x[0]) for x in a])\n",
        "\n",
        "new_pp = PredictorWrapper.wrap_predict(invert)\n",
        "\n",
        "# Get predictions\n",
        "predictions = custom_model.predict_pairs([('Who is smarter?', 'John is smart')])\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "71215b50b7d640ea87f33ed9a89d5ca7",
            "dda9abf914434f23b4650eac05e4b326",
            "4707a98d04244a0e8bc792d1125986ad",
            "d88faa99a73a4fd0862aa56befdbb56e",
            "1bb362f15ec0480cb91b2881d8c352a4",
            "9f0ed7fb3df044deb56831aa50178e6b",
            "ba219fc000ff4b3da0a1d01132ca48d1",
            "e1bfa94cf1f64b12b1f42104aff223f7",
            "b53f079a64ab432994da72af5078bfcf",
            "dae3705b51e948939053c0169e5dcfb6",
            "ea75fb7e35ef4877b825b2fd718c16d8",
            "588f49ed58fc4a4788b2423888ee66cf",
            "49e3850d33064d4b8e0286677ba8e3dc",
            "ce43df41acbb4e00b16bc59e1d71e90e",
            "589bd975ecc8413184928c40dcc43872",
            "55bf5f88d7694b21a5d9a2a202d56843",
            "b2558b92c6d04300b8a63e438d24e807",
            "c23d93168d1049df9f71c611eabad079",
            "fb55699475714dd0854f5745953b1c67",
            "d6476e9f752943ba8ca96c1b4f31233f",
            "1a644271841842868ca0f4610da3c83d",
            "55896e37b3d14712a550655953f2b61e",
            "ede27b2edd53440293bcd6195751c0c3",
            "c7b393bc9f14422aac957ab2ee39e35a",
            "e29ef15743f144dda5fce5d49a68444a",
            "707819e613bf40e198842f2c7e404fc5",
            "b6ab91f49ada47b89e5517ec47d1f212",
            "3bf17a8e9fbb4224bf3ec6198b901513",
            "c9a8908dfa5a4b159144e1ff602aeced",
            "dc539d15d5024f39abce5d01d1b9001e",
            "551efc9f74614bfe827cae45e308ecec",
            "9c94789d72194acf96225ba2866b1fb4",
            "26758fb09c334de6a401f82591900b54",
            "967e36a15d28467e9f11fae9d1c91727",
            "6c985ba3efda476f9817a660facfa4d2",
            "33f5c7e40ef448ed938fe4c15ed79159",
            "b9f645a68e2040e78915523b3b68d6d5",
            "0abc0f3efc9747ebb7cfcbeb17a7804a",
            "2d100756c45642faa3814150e3e93e3e",
            "0071a8d007f54283bd8760a6b82d8d86",
            "e15d30dd4d4b4ee9a6c7f0f040a7617e",
            "80e7633e1c1645d4ba76eaca30781709",
            "cd42c220d3fd449a99dc157ac2c983e1",
            "b4a58d6253f54af1a12dd49c588a7d6b"
          ]
        },
        "id": "gzcXQ4-lvcyD",
        "outputId": "812bd968-e925-4e91-89d2-71fad85f4f6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71215b50b7d640ea87f33ed9a89d5ca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "588f49ed58fc4a4788b2423888ee66cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ede27b2edd53440293bcd6195751c0c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "967e36a15d28467e9f11fae9d1c91727"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['John']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invert([('John is smart', 'Who is smart')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG1nKxYAxIe8",
        "outputId": "9606fa5a-3fa7-4e91-c9b0-6ee6ebb551e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "editor = checklist.editor.Editor()\n",
        "editor.tg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epclhS4J0SKx",
        "outputId": "919a8035-456b-4623-a1ed-e1407c6bca2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<checklist.text_generation.TextGenerator at 0x7f188cddca00>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "BJXjZWis0UqR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_squad_with_context(x, pred, conf, label=None, *args, **kwargs):\n",
        "    c, q = x\n",
        "    ret = 'C: %s\\nQ: %s\\n' % (c, q)\n",
        "    if label is not None:\n",
        "        ret += 'A: %s\\n' % label\n",
        "    ret += 'P: %s\\n' % pred\n",
        "    return ret"
      ],
      "metadata": {
        "id": "F-Y-KLy40XUZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_squad(x, pred, conf, label=None, *args, **kwargs):\n",
        "    c, q = x\n",
        "    ret = 'Q: %s\\n' % (q)\n",
        "    if label is not None:\n",
        "        ret += 'A: %s\\n' % label\n",
        "    ret += 'P: %s\\n' % pred\n",
        "    return ret"
      ],
      "metadata": {
        "id": "msp2fDTK0YKx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def load_squad(fold='validation'):\n",
        "    answers = []\n",
        "    data = []\n",
        "    ids = []\n",
        "    files = {\n",
        "        'validation': '/content/drive/MyDrive/TaCL-main/TaCL-main/datasets/dev-v1.1.json',\n",
        "        'train': '/content/drive/MyDrive/TaCL-main/TaCL-main/datasets/train-v1.1.json',\n",
        "        }\n",
        "    f = json.load(open(files[fold]))\n",
        "    for t in f['data']:\n",
        "        for p in t['paragraphs']:\n",
        "            context = p['context']\n",
        "            for qa in p['qas']:\n",
        "                data.append({'passage': context, 'question': qa['question'], 'id': qa['id']})\n",
        "                answers.append(set([(x['text'], x['answer_start']) for x in qa['answers']]))\n",
        "    return data, answers"
      ],
      "metadata": {
        "id": "KcddMK7r0zme"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "# from load_squad import load_squad\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "data, answers = load_squad()\n",
        "\n",
        "# Load the Spacy model\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "\n",
        "# Process the passages and questions using Spacy and save the mapping to a pickled file\n",
        "processed_squad = {}\n",
        "for i, d in enumerate(tqdm(data)):\n",
        "    processed_squad[d['passage']] = nlp(d['passage'])\n",
        "    processed_squad[d['question']] = nlp(d['question'])\n",
        "with open('processed_squad.pkl', 'wb') as f:\n",
        "    pickle.dump(processed_squad, f)\n"
      ],
      "metadata": {
        "id": "8TwV3TNC0_sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "data, answers =  load_squad()\n",
        "spacy_map =  pickle.load(open('/content/drive/MyDrive/TaCL-main/TaCL-main/checklist/processed_squad.pkl', 'rb'))\n",
        "pairs = [(x['passage'], x['question']) for x in data]\n",
        "processed_pairs = [(spacy_map[x[0]], spacy_map[x[1]]) for x in pairs]"
      ],
      "metadata": {
        "id": "19z_qkLJQaBZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suite = TestSuite()\n"
      ],
      "metadata": {
        "id": "_B0WZeX45Gfc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vocabulary"
      ],
      "metadata": {
        "id": "lrDLrzZMvyl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(', '.join(editor.suggest('{first_name} is {mask} than {first_name2}.')[:60]))\n",
        "adj = ['old', 'smart', 'tall', 'young', 'strong', 'short', 'tough', 'cool', 'fast', 'nice', 'small', 'dark', 'wise', 'rich', 'great', 'weak', 'high', 'slow', 'strange', 'clean']\n",
        "adj = [(x.rstrip('e'), x) for x in adj]\n",
        "adj[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVcd5XJL5KR0",
        "outputId": "7d233990-710e-46bb-9bc2-0ba046ea053c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/checklist/text_generation.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  to_pred = torch.tensor(to_pred, device=self.device).to(torch.int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smarter, older, better, younger, taller, worse, stronger, different, shorter, cooler, nicer, tougher, bigger, hotter, happier, smaller, wiser, more, faster, richer, darker, thinner, weaker, less, larger, quieter, cleaner, heavier, healthier, closer, colder, slower, wealthier, quicker, longer, harder, safer, lighter, warmer, brighter, cheaper, sharper, higher, louder, thicker, greater, lower, easier, deeper, poorer, softer, smoother, simpler, stranger, newer, other, superior, clearer, stricter, tighter\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tall', 'tall')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = editor.template(\n",
        "    [(\n",
        "    '{first_name} is {adj[0]}er than {first_name1}.',\n",
        "    'Who is less {adj[1]}?'\n",
        "    ),(\n",
        "    '{first_name} is {adj[0]}er than {first_name1}.',\n",
        "    'Who is {adj[0]}er?'\n",
        "    )\n",
        "    ],\n",
        "    labels = ['{first_name1}','{first_name}'],\n",
        "    adj=adj,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=100,\n",
        "    save=True\n",
        "    )\n",
        "name = 'A is COMP than B. Who is more / less COMP?'\n",
        "description = ''\n",
        "test = MFT(**t, name=name, description=description, capability='Vocabulary')\n",
        "test.run(new_pp)\n",
        "test.summary(n=10, format_example_fn=format_squad_with_context)\n",
        "suite.add(test,overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6YMIg865PBD",
        "outputId": "31b4e244-5dc4-47f1-a260-bbb2ccbc9706"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 200 examples\n",
            "Test cases:      100\n",
            "Fails (rate):    38 (38.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Victoria is shorter than Ralph.\n",
            "Q: Who is less short?\n",
            "A: Ralph\n",
            "P: Victoria\n",
            "\n",
            "\n",
            "----\n",
            "C: Kim is smaller than Emily.\n",
            "Q: Who is less small?\n",
            "A: Emily\n",
            "P: Kim\n",
            "\n",
            "\n",
            "----\n",
            "C: Henry is shorter than Katie.\n",
            "Q: Who is less short?\n",
            "A: Katie\n",
            "P: Henry is shorter than katie\n",
            "\n",
            "\n",
            "----\n",
            "C: Sharon is cleaner than Samuel.\n",
            "Q: Who is less clean?\n",
            "A: Samuel\n",
            "P: Sharon is cleaner than samuel\n",
            "\n",
            "\n",
            "----\n",
            "C: Catherine is shorter than Amy.\n",
            "Q: Who is less short?\n",
            "A: Amy\n",
            "P: Catherine\n",
            "\n",
            "\n",
            "----\n",
            "C: Jeff is younger than Alice.\n",
            "Q: Who is less young?\n",
            "A: Alice\n",
            "P: Jeff\n",
            "\n",
            "\n",
            "----\n",
            "C: Paul is darker than Katherine.\n",
            "Q: Who is less dark?\n",
            "A: Katherine\n",
            "P: Paul is darker than katherine\n",
            "\n",
            "\n",
            "----\n",
            "C: Sarah is higher than Steven.\n",
            "Q: Who is less high?\n",
            "A: Steven\n",
            "P: Sarah\n",
            "\n",
            "\n",
            "----\n",
            "C: Eleanor is greater than Victoria.\n",
            "Q: Who is less great?\n",
            "A: Victoria\n",
            "P: Eleanor is greater than victoria\n",
            "\n",
            "\n",
            "----\n",
            "C: Helen is shorter than Roger.\n",
            "Q: Who is less short?\n",
            "A: Roger\n",
            "P: Helen\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.summary(n=38, format_example_fn=format_squad_with_context)"
      ],
      "metadata": {
        "id": "Dvn9JkFW0RFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crossproduct(t):\n",
        "    # takes the output of editor.template and does the cross product of contexts and qas\n",
        "    ret = []\n",
        "    ret_labels = []\n",
        "    for x in t.data:\n",
        "        cs = x['contexts']\n",
        "        qas = x['qas']\n",
        "        d = list(itertools.product(cs, qas))\n",
        "        ret.append([(x[0], x[1][0]) for x in d])\n",
        "        ret_labels.append([x[1][1] for x in d])\n",
        "    t.data = ret\n",
        "    t.labels = ret_labels\n",
        "    return t"
      ],
      "metadata": {
        "id": "up5Chkqh_Iem"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = editor.suggest('John is very {mask} about the project.')[:20]\n",
        "print(', '.join(editor.suggest('John is {mask} {state} about the project.', state=state)[:30]))\n",
        "very = ['very', 'extremely', 'really', 'quite', 'incredibly', 'particularly', 'highly', 'super']\n",
        "somewhat = ['a little', 'somewhat', 'slightly', 'mildly']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WLlEkCn_Ky9",
        "outputId": "e3e01fef-8cda-4fe1-f3d4-90ae8e42c79f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/checklist/text_generation.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  to_pred = torch.tensor(to_pred, device=self.device).to(torch.int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "very, pretty, extremely, also, still, quite, more, really, not, clearly, fairly, incredibly, particularly, now, understandably, rather, cautiously, surprisingly, certainly, feeling, so, especially, definitely, generally, most, highly, super, reportedly, being, obviously\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is {very} {s} about the project. {first_name1} is {s} about the project.',\n",
        "            '{first_name1} is {s} about the project. {first_name} is {very} {s} about the project.',\n",
        "            '{first_name} is {s} about the project. {first_name1} is {somewhat} {s} about the project.',\n",
        "            '{first_name1} is {somewhat} {s} about the project. {first_name} is {s} about the project.',\n",
        "            '{first_name} is {very} {s} about the project. {first_name1} is {somewhat} {s} about the project.',\n",
        "            '{first_name1} is {somewhat} {s} about the project. {first_name} is {very} {s} about the project.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is most {s} about the project?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is least {s} about the project?',\n",
        "                '{first_name1}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    s = state,\n",
        "    very=very,\n",
        "    somewhat=somewhat,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=25,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'Intensifiers (very, super, extremely) and reducers (somewhat, kinda, etc)?'\n",
        "desc = ''\n",
        "test = MFT(**t, name=name, description=desc, capability='Vocabulary')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test,overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDDfUar6_ON8",
        "outputId": "03492a36-d982-4184-c995-cc0c73527720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 300 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "class CustomModel:\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "    def predict_pairs(self, pairs):\n",
        "        # Convert pairs into the required format for your model's input\n",
        "        inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Get the raw predictions from your model\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Get start and end logits\n",
        "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "        # Get start and end positions\n",
        "        start_positions = torch.argmax(start_logits, dim=1)\n",
        "        end_positions = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "        # Extract the answers\n",
        "        answers = []\n",
        "        for i, (start, end) in enumerate(zip(start_positions, end_positions)):\n",
        "            answer = self.tokenizer.decode(inputs[\"input_ids\"][i][start : end + 1], skip_special_tokens=True)\n",
        "            answers.append(answer)\n",
        "\n",
        "        # Return the list of predictions\n",
        "        return answers\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"sandeepvarma99/tacl-bert-squad-trained\"\n",
        "# model_name = \"bert-base-uncased\"\n",
        "custom_model = CustomModel(model_name)\n",
        "\n",
        "# Update the invert function to use the custom model's predict_pairs method\n",
        "invert = lambda a: custom_model.predict_pairs([(x[1], x[0]) for x in a])\n",
        "\n",
        "new_pp = PredictorWrapper.wrap_predict(invert)\n",
        "\n",
        "# Get predictions\n",
        "predictions = custom_model.predict_pairs([('Who is smarter?', 'John is smart')])\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxvcXnMvOk9u",
        "outputId": "296317da-f882-4117-eb95-a36f695156af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['John']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Taxonomy"
      ],
      "metadata": {
        "id": "_XsWcoupv5gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Size, chape, color, age, material"
      ],
      "metadata": {
        "id": "Lp55VQqRwMmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import munch\n",
        "order = ['size', 'shape', 'age', 'color']\n",
        "props = []\n",
        "properties = {\n",
        "    'color' : ['red', 'blue','yellow', 'green', 'pink', 'white', 'black', 'orange', 'grey', 'purple', 'brown'],\n",
        "    'size' : ['big', 'small', 'tiny', 'enormous'],\n",
        "    'age' : ['old', 'new'],\n",
        "    'shape' : ['round', 'oval', 'square', 'triangular'],\n",
        "    'material' : ['iron', 'wooden', 'ceramic', 'glass', 'stone']\n",
        "}\n",
        "for i in range(len(order)):\n",
        "    for j in range(i + 1, len(order)):\n",
        "        p1, p2 = order[i], order[j]\n",
        "        for v1, v2 in itertools.product(properties[p1], properties[p2]):\n",
        "            props.append(munch.Munch({\n",
        "                'p1': p1,\n",
        "                'p2': p2,\n",
        "                'v1': v1,\n",
        "                'v2': v2,\n",
        "            }))"
      ],
      "metadata": {
        "id": "xl9T7-QNJlfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(', '.join(editor.suggest('There is {a:p.v1} {p.v2} {mask} in the room.', p=props, verbose=False)[:30]))\n",
        "objects = ['box', 'clock', 'table', 'object', 'toy', 'painting', 'sculpture', 'thing', 'figure']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1llxxbrJrtx",
        "outputId": "a96df18b-d1a1-460e-d279-3b282b538225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sofa, couch, wall, carpet, chair, table, light, lamp, door, clock, mirror, desk, bed, TV, bar, television, window, box, tree, painting, curtain, fan, fridge, screen, wallpaper, piano, rug, shelf, camera, candle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            'There is {a:p.v1} {p.v2} {obj} in the room.',\n",
        "            'There is {a:obj} in the room. The {obj} is {p.v1} and {p.v2}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'What {p.p1} is the {obj}?',\n",
        "                '{p.v1}'\n",
        "            ), \n",
        "            (\n",
        "                'What {p.p2} is the {obj}?',\n",
        "                '{p.v2}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    obj=objects,\n",
        "    p=props,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'size, shape, age, color'\n",
        "desc = ''\n",
        "test = MFT(**t, name=name, description=desc, capability='Taxonomy')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test,overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmf_vn0rJvMi",
        "outputId": "3e5fb3e2-cf10-4ede-fe6c-e5af19119812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 20 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    4 (80.0%)\n",
            "\n",
            "Example fails:\n",
            "C: There is a big black clock in the room.\n",
            "Q: What size is the clock?\n",
            "A: big\n",
            "P: big black\n",
            "\n",
            "\n",
            "----\n",
            "C: There is an old blue painting in the room.\n",
            "Q: What age is the painting?\n",
            "A: old\n",
            "P: old blue\n",
            "\n",
            "C: There is a painting in the room. The painting is old and blue.\n",
            "Q: What age is the painting?\n",
            "A: old\n",
            "P: old and blue\n",
            "\n",
            "C: There is a painting in the room. The painting is old and blue.\n",
            "Q: What color is the painting?\n",
            "A: blue\n",
            "P: old and blue\n",
            "\n",
            "\n",
            "----\n",
            "C: There is a tiny round thing in the room.\n",
            "Q: What size is the thing?\n",
            "A: tiny\n",
            "P: tiny round\n",
            "\n",
            "C: There is a thing in the room. The thing is tiny and round.\n",
            "Q: What size is the thing?\n",
            "A: tiny\n",
            "P: tiny and round\n",
            "\n",
            "C: There is a thing in the room. The thing is tiny and round.\n",
            "Q: What shape is the thing?\n",
            "A: round\n",
            "P: tiny and round\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Professions vs nationalities"
      ],
      "metadata": {
        "id": "0chzlyHZwTp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "professions = editor.suggest('{first_name} works as {a:mask}.')[:30]\n",
        "professions += editor.suggest('{first_name} {last_name} works as {a:mask}.')[:30]\n",
        "professions = list(set(professions))\n",
        "if 'translator' in professions:\n",
        "    professions.remove('translator')"
      ],
      "metadata": {
        "id": "7uIh5o7OJ45D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(string):\n",
        "    return string.lstrip('[a,the,an,in,at] ').rstrip('.')"
      ],
      "metadata": {
        "id": "p27F-MzmJ6LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expect_squad(x, pred, conf, label=None, meta=None):\n",
        "    return clean(pred) == clean(label)\n",
        "expect_squad = Expect.single(expect_squad)"
      ],
      "metadata": {
        "id": "VcXsvldVJ9Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is {a:nat} {prof}.',\n",
        "            '{first_name} is {a:prof}. {first_name} is {nat}.',\n",
        "            '{first_name} is {nat}. {first_name} is {a:prof}.',\n",
        "            '{first_name} is {nat} and {a:prof}.',\n",
        "            '{first_name} is {a:prof} and {nat}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'What is {first_name}\\'s job?',\n",
        "                '{prof}'\n",
        "            ), \n",
        "            (\n",
        "                'What is {first_name}\\'s nationality?',\n",
        "                '{nat}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    nat = editor.lexicons['nationality'][:10],\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True,\n",
        "    ))\n",
        "name = 'Profession vs nationality'\n",
        "test = MFT(**t, name=name, expect=expect_squad, description='',  capability='Taxonomy')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test,overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIWaDwRaJ_Ut",
        "outputId": "c47f2096-771f-407c-b9e1-36af50783c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 50 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    3 (60.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Florence is an Indian intern.\n",
            "Q: What is Florence's job?\n",
            "A: intern\n",
            "P: Indian intern\n",
            "\n",
            "\n",
            "----\n",
            "C: Henry is a Chinese educator.\n",
            "Q: What is Henry's job?\n",
            "A: educator\n",
            "P: Chinese educator\n",
            "\n",
            "\n",
            "----\n",
            "C: Dave is a Japanese author.\n",
            "Q: What is Dave's job?\n",
            "A: author\n",
            "P: Japanese author\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Animal vs vehicle"
      ],
      "metadata": {
        "id": "2wagvmdnwY7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animals = ['dog', 'cat', 'bull', 'cow', 'fish', 'serpent', 'snake', 'lizard', 'hamster', 'rabbit', 'guinea pig', 'iguana', 'duck']\n",
        "vehicles = ['car', 'truck', 'train', 'motorcycle', 'bike', 'firetruck', 'tractor', 'van', 'SUV', 'minivan']\n",
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} has {a:animal} and {a:vehicle}.',\n",
        "            '{first_name} has {a:vehicle} and {a:animal}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'What animal does {first_name} have?',\n",
        "                '{animal}'\n",
        "            ), \n",
        "            (\n",
        "                'What vehicle does {first_name} have?',\n",
        "                '{vehicle}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    animal=animals,\n",
        "    vehicle=vehicles,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'Animal vs Vehicle'\n",
        "test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test, overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve5ahURnPD4D",
        "outputId": "ea034c5c-bea6-4aba-d5cf-24f4f7350a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 20 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    0 (0.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = ['dog', 'cat', 'bull', 'cow', 'fish', 'serpent', 'snake', 'lizard', 'hamster', 'rabbit', 'guinea pig', 'iguana', 'duck']\n",
        "vehicles = ['car', 'truck', 'train', 'motorcycle', 'bike', 'firetruck', 'tractor', 'van', 'SUV', 'minivan']\n",
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} bought {a:animal}. {first_name2} bought {a:vehicle}.',\n",
        "            '{first_name2} bought {a:vehicle}. {first_name} bought {a:animal}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who bought an animal?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who bought a vehicle?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    animal=animals,\n",
        "    vehicle=vehicles,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'Animal vs Vehicle v2'\n",
        "test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test, overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmxuTsRaPV1H",
        "outputId": "e62c934c-2c90-4058-f290-dfacd8f2af0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 20 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    3 (60.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Sandra bought a firetruck. Elizabeth bought a duck.\n",
            "Q: Who bought an animal?\n",
            "A: Elizabeth\n",
            "P: Sandra\n",
            "\n",
            "\n",
            "----\n",
            "C: Roy bought a bike. Jean bought a cow.\n",
            "Q: Who bought an animal?\n",
            "A: Jean\n",
            "P: Roy\n",
            "\n",
            "\n",
            "----\n",
            "C: Don bought a guinea pig. Fiona bought a tractor.\n",
            "Q: Who bought a vehicle?\n",
            "A: Fiona\n",
            "P: Don\n",
            "\n",
            "C: Fiona bought a tractor. Don bought a guinea pig.\n",
            "Q: Who bought an animal?\n",
            "A: Don\n",
            "P: Fiona bought a tractor. Don\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = [ ('spiritual', 'religious'), ('angry', 'furious'), ('organized', 'organised'),\n",
        "            ('vocal', 'outspoken'), ('grateful', 'thankful'), ('intelligent', 'smart'),\n",
        "            ('humble', 'modest'), ('courageous', 'brave'), ('happy', 'joyful'), ('scared', 'frightened'),\n",
        "           ]\n",
        "\n",
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is very {s1[0]}. {first_name2} is very {s2[0]}.',\n",
        "            '{first_name2} is very {s2[0]}. {first_name} is very {s1[0]}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {s1[1]}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {s2[1]}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    s=synonyms,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "   ))\n",
        "t += crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is very {s1[1]}. {first_name2} is very {s2[1]}.',\n",
        "            '{first_name2} is very {s2[1]}. {first_name} is very {s1[1]}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {s1[0]}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {s2[0]}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    s=synonyms,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "    )) \n",
        "name = 'Synonyms'\n",
        "test = MFT(**t, name=name, description='', capability='Taxonomy', expect=expect_squad)\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsJ7Ys1QPb0f",
        "outputId": "9180ecff-de76-4cb2-b546-bcd81efbb93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 36 examples\n",
            "Test cases:      9\n",
            "Fails (rate):    0 (0.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comp_pairs = [('better', 'worse'), ('older', 'younger'), ('smarter', 'dumber'), ('taller', 'shorter'), ('bigger', 'smaller'), ('stronger', 'weaker'), ('faster', 'slower'), ('darker', 'lighter'), ('richer', 'poorer'), ('happier', 'sadder'), ('louder', 'quieter'), ('warmer', 'colder')]\n",
        "comp_pairs = list(set(comp_pairs))#list(set(comp_pairs + [(x[1], x[0]) for x in comp_pairs]))"
      ],
      "metadata": {
        "id": "RsVEqDrNPnfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is {comp[0]} than {first_name1}.',\n",
        "            '{first_name1} is {comp[1]} than {first_name}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {comp[1]}?',\n",
        "                '{first_name1}',\n",
        "            ),\n",
        "            (\n",
        "                'Who is {comp[0]}?',\n",
        "                '{first_name}',\n",
        "            )\n",
        "            \n",
        "        ]\n",
        "        ,\n",
        "    },\n",
        "    comp=comp_pairs,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'A is COMP than B. Who is antonym(COMP)? B'\n",
        "test = MFT(**t, name=name, description='', capability='Taxonomy')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2PgNPqOPobK",
        "outputId": "8bfd4038-d354-4260-9ed8-472acf09f1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 20 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    5 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Cynthia is older than Stephanie.\n",
            "Q: Who is younger?\n",
            "A: Stephanie\n",
            "P: Cynthia is older than Stephanie\n",
            "\n",
            "\n",
            "----\n",
            "C: Thomas is poorer than Katie.\n",
            "Q: Who is richer?\n",
            "A: Katie\n",
            "P: Thomas is poorer than Katie\n",
            "\n",
            "\n",
            "----\n",
            "C: Rachel is older than Frederick.\n",
            "Q: Who is younger?\n",
            "A: Frederick\n",
            "P: Rachel\n",
            "\n",
            "C: Frederick is younger than Rachel.\n",
            "Q: Who is older?\n",
            "A: Rachel\n",
            "P: Frederick\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "antonym_adjs = [('progressive', 'conservative'),('religious', 'secular'),('positive', 'negative'),('defensive', 'offensive'),('rude',  'polite'),('optimistic', 'pessimistic'),('stupid', 'smart'),('negative', 'positive'),('unhappy', 'happy'),('active', 'passive'),('impatient', 'patient'),('powerless', 'powerful'),('visible', 'invisible'),('fat', 'thin'),('bad', 'good'),('cautious', 'brave'), ('hopeful', 'hopeless'),('insecure', 'secure'),('humble', 'proud'),('passive', 'active'),('dependent', 'independent'),('pessimistic', 'optimistic'),('irresponsible', 'responsible'),('courageous', 'fearful')]\n",
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is more {a[0]} than {first_name1}.',\n",
        "            '{first_name1} is more {a[1]} than {first_name}.',\n",
        "            '{first_name} is less {a[1]} than {first_name1}.',\n",
        "            '{first_name1} is less {a[0]} than {first_name}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is more {a[0]}?',\n",
        "                '{first_name}',\n",
        "            ),\n",
        "            (\n",
        "                'Who is less {a[0]}?',\n",
        "                '{first_name1}',\n",
        "            ),\n",
        "            (\n",
        "                'Who is more {a[1]}?',\n",
        "                '{first_name1}',\n",
        "            ),\n",
        "            (\n",
        "                'Who is less {a[1]}?',\n",
        "                '{first_name}',\n",
        "            ),\n",
        "        ]\n",
        "        ,\n",
        "    },\n",
        "    a = antonym_adjs,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=5,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'A is more X than B. Who is more antonym(X)? B. Who is less X? B. Who is more X? A. Who is less antonym(X)? A.'\n",
        "test = MFT(**t, name=name, description='', capability='Taxonomy')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSGynCRzPuOp",
        "outputId": "f6a266ce-2a91-4af2-c338-c3fda13e6b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 80 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    5 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Julia is more humble than Caroline.\n",
            "Q: Who is more proud?\n",
            "A: Caroline\n",
            "P: Julia\n",
            "\n",
            "C: Julia is more humble than Caroline.\n",
            "Q: Who is less proud?\n",
            "A: Julia\n",
            "P: Caroline\n",
            "\n",
            "C: Caroline is more proud than Julia.\n",
            "Q: Who is less humble?\n",
            "A: Caroline\n",
            "P: Julia\n",
            "\n",
            "\n",
            "----\n",
            "C: Victoria is more rude than Roy.\n",
            "Q: Who is less polite?\n",
            "A: Victoria\n",
            "P: Roy\n",
            "\n",
            "C: Roy is more polite than Victoria.\n",
            "Q: Who is more rude?\n",
            "A: Victoria\n",
            "P: Roy\n",
            "\n",
            "C: Victoria is less polite than Roy.\n",
            "Q: Who is more rude?\n",
            "A: Victoria\n",
            "P: Roy\n",
            "\n",
            "\n",
            "----\n",
            "C: Virginia is more unhappy than Thomas.\n",
            "Q: Who is more happy?\n",
            "A: Thomas\n",
            "P: Virginia\n",
            "\n",
            "C: Thomas is more happy than Virginia.\n",
            "Q: Who is more unhappy?\n",
            "A: Virginia\n",
            "P: Thomas\n",
            "\n",
            "C: Thomas is more happy than Virginia.\n",
            "Q: Who is less unhappy?\n",
            "A: Thomas\n",
            "P: Virginia\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Robustness"
      ],
      "metadata": {
        "id": "QnT_UIJ1wlBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def question_typo(x):\n",
        "    return (x[0], Perturb.add_typos(x[1]))\n",
        "t = Perturb.perturb(pairs, question_typo, nsamples=5)\n",
        "test = INV(**t, name='Question typo', capability='Robustness', description='')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad)\n",
        "suite.add(test, overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ujTe9sBP3hI",
        "outputId": "5634576a-813e-436b-a33c-f2a01f0bed66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 10 examples\n",
            "Test cases:      5\n",
            "Fails (rate):    0 (0.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contractions(x):\n",
        "    conts = Perturb.contractions(x[1])\n",
        "    return [(x[0], a) for a in conts]\n",
        "t = Perturb.perturb(pairs, contractions, nsamples=2)\n",
        "test = INV(**t, name='Question contractions', capability='Robustness', description='')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frnoWAYGQ7os",
        "outputId": "ec5c31fa-a012-4922-eb68-5cc5d5938410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 4 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    1 (50.0%)\n",
            "\n",
            "Example fails:\n",
            "Q: What is a mechanism that can help plants block virus replication?\n",
            "P: RNA silencing mechanisms\n",
            "\n",
            "Q: What's a mechanism that can help plants block virus replication?\n",
            "P: RNA silencing\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sentences = set()\n",
        "for x, _ in processed_pairs:\n",
        "    for y in x.sents:\n",
        "        random_sentences.add(y.text)\n",
        "random_sentences = list(random_sentences)"
      ],
      "metadata": {
        "id": "LqQjg6_ZQCWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_random_sentence(x, **kwargs):\n",
        "    random_s = np.random.choice(random_sentences)\n",
        "    while random_s in x[0]:\n",
        "        random_s = np.random.choice(random_sentences)\n",
        "    random_s = random_s.strip('.') + '. '\n",
        "    meta = ['add to end: %s' % random_s, 'add to beg: %s' % random_s]\n",
        "    return [(x[0] + random_s, x[1]), (random_s + x[0], x[1])], meta\n",
        "\n",
        "def format_add(x, pred, conf, label=None, meta=None):\n",
        "    ret = format_squad(x, pred, conf, label, meta)\n",
        "    if meta:\n",
        "        ret += 'Perturb: %s\\n' % meta\n",
        "    return ret\n",
        "\n",
        "t = Perturb.perturb(pairs, add_random_sentence, nsamples=2, meta=True)\n",
        "test = INV(**t, name='Add random sentence to context', capability='Robustness', description='')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_add)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bR4Po-wRnvQ",
        "outputId": "abb8f8ec-2aab-4ef1-cefa-a9fd96962428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 6 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    0 (0.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NER"
      ],
      "metadata": {
        "id": "PbS0mge0w6Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def change_thing(change_fn):\n",
        "    def change_both(cq, **kwargs):\n",
        "        context, question = cq\n",
        "        a = change_fn(context, meta=True)\n",
        "        if not a:\n",
        "            return None\n",
        "        changed, meta = a\n",
        "        ret = []\n",
        "        for c, m in zip(changed, meta):\n",
        "            new_q = re.sub(r'\\b%s\\b' % re.escape(m[0]), m[1], question.text)\n",
        "            ret.append((c, new_q))\n",
        "        return ret, meta\n",
        "    return change_both\n",
        "            "
      ],
      "metadata": {
        "id": "MeUEqpnqSPDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expect_same(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
        "    if not meta:\n",
        "        return pred == orig_pred\n",
        "    return pred == re.sub(r'\\b%s\\b' % re.escape(meta[0]), meta[1], orig_pred)\n",
        "\n",
        "def format_replace(x, pred, conf, label=None, meta=None):\n",
        "    ret = format_squad(x, pred, conf, label, meta)\n",
        "    if meta:\n",
        "        ret += 'Perturb: %s -> %s\\n' % meta\n",
        "    return ret\n",
        "\n",
        "def format_replace_context(x, pred, conf, label=None, meta=None):\n",
        "    ret = format_squad_with_context(x, pred, conf, label, meta)\n",
        "    if meta:\n",
        "        ret += 'Perturb: %s -> %s\\n' % meta\n",
        "    return ret"
      ],
      "metadata": {
        "id": "x2wTiOJ2S5YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Perturb.perturb(processed_pairs, change_thing(Perturb.change_names), nsamples=2, meta=True)\n",
        "\n",
        "test = INV(**t, name='Change name everywhere', capability='NER',\n",
        "          description='', expect=Expect.pairwise(expect_same))\n",
        "test.run(new_pp)\n",
        "test.summary(3, format_example_fn=format_replace)\n",
        "suite.add(test, overwrite=True)"
      ],
      "metadata": {
        "id": "vyY4ply3S7-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Perturb.perturb(processed_pairs, change_thing(Perturb.change_location), nsamples=1, meta=True)\n",
        "\n",
        "test = INV(**t, name='Change location everywhere', capability='NER',\n",
        "          description='', expect=Expect.pairwise(expect_same))\n",
        "test.run(new_pp)\n",
        "test.summary(3, format_example_fn=format_replace)\n",
        "suite.add(test, overwrite=True)"
      ],
      "metadata": {
        "id": "2eauTBTUTgZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Temporal"
      ],
      "metadata": {
        "id": "dLDYaPNRw2F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            'Both {first_name} and {first_name2} were {prof1}s, but there was a change in {first_name}, who is now {a:prof2}.',\n",
        "            'Both {first_name2} and {first_name} were {prof1}s, but there was a change in {first_name}, who is now {a:prof2}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {a:prof2}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "name = 'There was a change in profession'\n",
        "test = MFT(**t, expect=expect_squad, capability='Temporal', name=name, description='' )\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dArn_HggT7Qe",
        "outputId": "6a4cefa9-dbe4-4d76-9f30-7141219d195b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 4 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Both Scott and Alex were accountants, but there was a change in Scott, who is now an escort.\n",
            "Q: Who is an escort?\n",
            "A: Scott\n",
            "P: Scott and Alex were accountants, but there was a change in Scott\n",
            "\n",
            "C: Both Alex and Scott were accountants, but there was a change in Scott, who is now an escort.\n",
            "Q: Who is an escort?\n",
            "A: Scott\n",
            "P: Scott were accountants, but there was a change in Scott\n",
            "\n",
            "\n",
            "----\n",
            "C: Both Marie and Robin were interpreters, but there was a change in Robin, who is now an agent.\n",
            "Q: Who is an agent?\n",
            "A: Robin\n",
            "P: Robin were interpreters, but there was a change in Robin\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} became a {prof} before {first_name2} did.',\n",
        "            '{first_name2} became a {prof} after {first_name} did.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who became a {prof} first?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who became a {prof} last?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "name = 'Understanding before / after -> first / last.'\n",
        "test = MFT(**t, expect=expect_squad, capability='Temporal', name=name, description='' )\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQxAPtPlUcuJ",
        "outputId": "527a3841-dfb8-44d3-cb6c-cac71699ef02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 8 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Carolyn became a waitress before Alexander did.\n",
            "Q: Who became a waitress last?\n",
            "A: Alexander\n",
            "P: Carolyn\n",
            "\n",
            "C: Alexander became a waitress after Carolyn did.\n",
            "Q: Who became a waitress last?\n",
            "A: Alexander\n",
            "P: Carolyn\n",
            "\n",
            "\n",
            "----\n",
            "C: Kathy became a economist before Nick did.\n",
            "Q: Who became a economist last?\n",
            "A: Nick\n",
            "P: Kathy\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Negation"
      ],
      "metadata": {
        "id": "qjFNZC6ixAxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is not {a:prof}. {first_name2} is.',\n",
        "            '{first_name2} is {a:prof}. {first_name} is not.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {a:prof}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is not {a:prof}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "name = 'Negation in context, may or may not be in question'\n",
        "test = MFT(**t, expect=expect_squad, capability='Negation', name=name, description='' )\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Him091G3Uhwd",
        "outputId": "be2022b8-3cff-4c31-ddec-2546d51673a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 8 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    1 (50.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Heather is not an investor. Arthur is.\n",
            "Q: Who is an investor?\n",
            "A: Arthur\n",
            "P: Heather\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} is {a:prof}. {first_name2} is {a:prof2}.',\n",
        "            '{first_name2} is {a:prof2}. {first_name} is {a:prof}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {a:prof}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is not {a:prof}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {a:prof2}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is not {a:prof2}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "name = 'Negation in question only.'\n",
        "test = MFT(**t, expect=expect_squad, capability='Negation', name=name, description='' )\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y19i8O3uUoDm",
        "outputId": "7c621177-a028-41e2-eda8-945b3337f5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 16 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Jane is an analyst. Andrew is an interpreter.\n",
            "Q: Who is not an analyst?\n",
            "A: Andrew\n",
            "P: Jane\n",
            "\n",
            "C: Jane is an analyst. Andrew is an interpreter.\n",
            "Q: Who is not an interpreter?\n",
            "A: Jane\n",
            "P: Andrew\n",
            "\n",
            "C: Andrew is an interpreter. Jane is an analyst.\n",
            "Q: Who is not an analyst?\n",
            "A: Andrew\n",
            "P: Jane\n",
            "\n",
            "\n",
            "----\n",
            "C: Sophie is an economist. Martin is an attorney.\n",
            "Q: Who is not an economist?\n",
            "A: Martin\n",
            "P: Sophie\n",
            "\n",
            "C: Sophie is an economist. Martin is an attorney.\n",
            "Q: Who is not an attorney?\n",
            "A: Sophie\n",
            "P: Martin\n",
            "\n",
            "C: Martin is an attorney. Sophie is an economist.\n",
            "Q: Who is not an economist?\n",
            "A: Martin\n",
            "P: Sophie\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fairness spinoff"
      ],
      "metadata": {
        "id": "BZDPeKrlxGxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "fewer_profs = ['doctor', 'nurse', 'secretary', 'CEO']\n",
        "t = editor.template(\n",
        "    [\n",
        "        ('{male} is not {a:prof}, {female} is.', 'Who is {a:prof}?', '{female}', 'woman', '{prof}'),\n",
        "        ('{female} is not {a:prof}, {male} is.', 'Who is {a:prof}?', '{male}', 'man', '{prof}'),\n",
        "    ],\n",
        "#     prof=professions + ['doctor'],\n",
        "    prof=fewer_profs,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=10,\n",
        "    unroll=True,\n",
        "    save=True,\n",
        "    )\n",
        "data = [(d[0], d[1]) for d in t.data]\n",
        "labels = [d[2] for d in t.data]\n",
        "meta = [(d[3], d[4]) for d in t.data]\n",
        "\n",
        "test = MFT(data, expect=expect_squad, labels=labels, meta=meta, templates=t.templates,\n",
        "          name='M/F failure rates should be similar for different professions', capability='Fairness',\n",
        "          description='Using negation in context.')\n",
        "test.run(new_pp)\n",
        "\n",
        "def print_fair(test):\n",
        "    c = collections.Counter(test.meta)\n",
        "    fail = collections.Counter([tuple(x) for x in np.array(test.meta)[test.fail_idxs()]])\n",
        "    profs = set()\n",
        "    for sex, prof in fail:\n",
        "        profs.add(prof)\n",
        "    prof_fail = {}\n",
        "    get_fail = lambda f:fail[f] / c[f]\n",
        "    for prof in profs:\n",
        "        fail_m = get_fail(('man', prof))\n",
        "        fail_f = get_fail(('woman', prof))\n",
        "        prof_fail[prof] = (fail_m, fail_f)\n",
        "    print('%-13s fail_men fail_women (count)' % 'profession')\n",
        "    for prof, vs in sorted(prof_fail.items(), key=lambda x:max(x[1][0], x[1][1]), reverse=True):\n",
        "        fail_m, fail_f = vs\n",
        "        print('%-13s   %.1f      %.1f     (%d)' % (prof, 100 * fail_m, 100 * fail_f, c[('man', prof)]))\n",
        "print_fair(test)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjhVycy9UvBJ",
        "outputId": "c0daa75a-de81-44ce-dc79-729883ff926c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 20 examples\n",
            "profession    fail_men fail_women (count)\n",
            "CEO             0.0      100.0     (4)\n",
            "nurse           40.0      40.0     (5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Coref"
      ],
      "metadata": {
        "id": "mEjw4OT9xNMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'actress' in professions:\n",
        "    professions.remove('actress')"
      ],
      "metadata": {
        "id": "RjfM1l5QU-B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{male} and {female} are friends. He is {a:prof1}, and she is {a:prof2}.',\n",
        "            '{female} and {male} are friends. He is {a:prof1}, and she is {a:prof2}.',\n",
        "            '{male} and {female} are friends. She is {a:prof2}, and he is {a:prof1}.',\n",
        "            '{female} and {male} are friends. She is {a:prof2}, and he is {a:prof1}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {a:prof1}?',\n",
        "                '{male}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {a:prof2}?',\n",
        "                '{female}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "name = 'Basic coref, he / she'\n",
        "test = MFT(**t, expect=expect_squad, name=name, description='', capability='Coref')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwmXqgbFVAvo",
        "outputId": "2308f622-923e-431e-80c1-912306a067bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 16 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Eric and Carolyn are friends. He is a model, and she is a DJ.\n",
            "Q: Who is a model?\n",
            "A: Eric\n",
            "P: Eric and Carolyn\n",
            "\n",
            "C: Carolyn and Eric are friends. He is a model, and she is a DJ.\n",
            "Q: Who is a DJ?\n",
            "A: Carolyn\n",
            "P: Eric\n",
            "\n",
            "C: Eric and Carolyn are friends. She is a DJ, and he is a model.\n",
            "Q: Who is a model?\n",
            "A: Eric\n",
            "P: Carolyn\n",
            "\n",
            "\n",
            "----\n",
            "C: Walter and Annie are friends. He is a secretary, and she is an architect.\n",
            "Q: Who is a secretary?\n",
            "A: Walter\n",
            "P: Walter and Annie\n",
            "\n",
            "C: Annie and Walter are friends. He is a secretary, and she is an architect.\n",
            "Q: Who is an architect?\n",
            "A: Annie\n",
            "P: Walter\n",
            "\n",
            "C: Walter and Annie are friends. She is an architect, and he is a secretary.\n",
            "Q: Who is a secretary?\n",
            "A: Walter\n",
            "P: Annie\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{male} and {female} are friends. His mom is {a:prof}.',\n",
        "            '{female} and {male} are friends. His mom is {a:prof}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Whose mom is {a:prof}?',\n",
        "                '{male}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "t += crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{male} and {female} are friends. Her mom is {a:prof}.',\n",
        "            '{female} and {male} are friends. Her mom is {a:prof}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Whose mom is {a:prof}?',\n",
        "                '{female}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "\n",
        "name = 'Basic coref, his / her'\n",
        "test = MFT(**t, expect=expect_squad, name=name, description='', capability='Coref')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXXBL_KYVFI3",
        "outputId": "0ee40f39-4c86-41fc-e2b9-5d6b9fabb82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 8 examples\n",
            "Test cases:      4\n",
            "Fails (rate):    4 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Richard and Melissa are friends. His mom is an editor.\n",
            "Q: Whose mom is an editor?\n",
            "A: Richard\n",
            "P: Melissa\n",
            "\n",
            "\n",
            "----\n",
            "C: Fiona and Louis are friends. Her mom is an editor.\n",
            "Q: Whose mom is an editor?\n",
            "A: Fiona\n",
            "P: Fiona and Louis\n",
            "\n",
            "\n",
            "----\n",
            "C: Anna and Roger are friends. Her mom is an editor.\n",
            "Q: Whose mom is an editor?\n",
            "A: Anna\n",
            "P: Roger\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} and {first_name2} are friends. The former is {a:prof1}.',\n",
        "            '{first_name2} and {first_name} are friends. The latter is {a:prof1}.',\n",
        "            '{first_name} and {first_name2} are friends. The former is {a:prof1} and the latter is {a:prof2}.',\n",
        "            '{first_name2} and {first_name} are friends. The former is {a:prof2} and the latter is {a:prof1}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who is {a:prof1}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    prof=professions,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    save=True\n",
        "    ))\n",
        "name = 'Former / Latter'\n",
        "test = MFT(**t, expect=expect_squad, name=name, description='', capability='Coref')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYXwiiZ3VrrB",
        "outputId": "86893a0f-0688-4594-870e-1d4e03ae6c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 8 examples\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Sophie and Rebecca are friends. The former is an editor.\n",
            "Q: Who is an editor?\n",
            "A: Sophie\n",
            "P: Rebecca\n",
            "\n",
            "C: Sophie and Rebecca are friends. The former is an editor and the latter is an author.\n",
            "Q: Who is an editor?\n",
            "A: Sophie\n",
            "P: Sophie and Rebecca\n",
            "\n",
            "\n",
            "----\n",
            "C: Wendy and Karen are friends. The former is an executive.\n",
            "Q: Who is an executive?\n",
            "A: Wendy\n",
            "P: Karen\n",
            "\n",
            "C: Wendy and Karen are friends. The former is an executive and the latter is an organizer.\n",
            "Q: Who is an executive?\n",
            "A: Wendy\n",
            "P: Karen\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SRL"
      ],
      "metadata": {
        "id": "ccNbgKRFxS9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pattern\n",
        "import pattern.en\n",
        "pverb = ['love', 'hate', 'like', 'remember', 'recognize', 'trust', 'deserve', 'understand', 'blame', 'dislike', 'prefer', 'follow', 'notice', 'hurt', 'bother', 'support', 'believe', 'accept', 'attack']\n",
        "a = pattern.en.tenses('loves')[0]\n",
        "b = pattern.en.tenses('stolen')[0]\n",
        "pverb = [(pattern.en.conjugate(v, *a), pattern.en.conjugate(v, *b)) for v in pverb]\n",
        "\n",
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} {v[0]} {first_name2}.',\n",
        "            '{first_name2} is {v[1]} by {first_name}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who {v[0]}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {v[1]}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    v=pverb,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=2,\n",
        "    ))\n",
        "name = 'Agent / object distinction'\n",
        "test = MFT(**t, expect=expect_squad, name=name, description='', capability='SRL')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "id": "azTuI8v2V0pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = crossproduct(editor.template(\n",
        "    {\n",
        "        'contexts': [\n",
        "            '{first_name} {v[0]} {first_name2}. {first_name2} {v[0]} {first_name3}.',\n",
        "            '{first_name} {v[0]} {first_name2}. {first_name3} is {v[1]} by {first_name2}.',\n",
        "            '{first_name2} is {v[1]} by {first_name}. {first_name2} {v[0]} {first_name3}.',\n",
        "            '{first_name2} is {v[1]} by {first_name}. {first_name3} is {v[1]} by {first_name2}.',\n",
        "        ],\n",
        "        'qas': [\n",
        "            (\n",
        "                'Who {v[0]} {first_name2}?',\n",
        "                '{first_name}'\n",
        "            ), \n",
        "            (\n",
        "                'Who {v[0]} {first_name3}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {v[1]} by {first_name}?',\n",
        "                '{first_name2}'\n",
        "            ), \n",
        "            (\n",
        "                'Who is {v[1]} by {first_name2}?',\n",
        "                '{first_name3}'\n",
        "            ), \n",
        "        ]\n",
        "        \n",
        "    },\n",
        "    save=True,\n",
        "    v=pverb,\n",
        "    remove_duplicates=True,\n",
        "    nsamples=1,\n",
        "    ))\n",
        "name = 'Agent / object distinction with 3 agents'\n",
        "test = MFT(**t, expect=expect_squad, name=name, description='', capability='SRL')\n",
        "test.run(new_pp)\n",
        "test.summary(n=3, format_example_fn=format_squad_with_context)\n",
        "suite.add(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJb9d9zRWDSC",
        "outputId": "3008fdd5-bfb4-4af7-dc77-f23d9cd76d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 16 examples\n",
            "Test cases:      1\n",
            "Fails (rate):    1 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Bob p Judy. Judy p Melissa.\n",
            "Q: Who p Judy?\n",
            "A: Bob\n",
            "P: Bob p Judy. Judy p Melissa\n",
            "\n",
            "C: Bob p Judy. Melissa is r by Judy.\n",
            "Q: Who p Melissa?\n",
            "A: Judy\n",
            "P: Bob\n",
            "\n",
            "C: Bob p Judy. Melissa is r by Judy.\n",
            "Q: Who is r by Bob?\n",
            "A: Judy\n",
            "P: Melissa\n",
            "\n",
            "\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suite.summary(n=3, format_example_fn=format_squad_with_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8MoQil_WJwR",
        "outputId": "23d4fc02-5c5e-46e0-8243-7bc2e8f43f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robustness\n",
            "\n",
            "Question contractions\n",
            "Test cases:      2\n",
            "Fails (rate):    1 (50.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Unlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant. Individual plant cells respond to molecules associated with pathogens known as Pathogen-associated molecular patterns or PAMPs. When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance (SAR) is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent. RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.\n",
            "Q: What is a mechanism that can help plants block virus replication?\n",
            "P: RNA silencing mechanisms\n",
            "\n",
            "C: Unlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant. Individual plant cells respond to molecules associated with pathogens known as Pathogen-associated molecular patterns or PAMPs. When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance (SAR) is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent. RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.\n",
            "Q: What's a mechanism that can help plants block virus replication?\n",
            "P: RNA silencing\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "Add random sentence to context\n",
            "Test cases:      2\n",
            "Fails (rate):    0 (0.0%)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fairness\n",
            "\n",
            "M/F failure rates should be similar for different professions\n",
            "Test cases:      20\n",
            "Fails (rate):    8 (40.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Frances is not a nurse, Dick is.\n",
            "Q: Who is a nurse?\n",
            "A: Dick\n",
            "P: Frances\n",
            "\n",
            "----\n",
            "C: Matt is not a CEO, Harriet is.\n",
            "Q: Who is a CEO?\n",
            "A: Harriet\n",
            "P: Matt\n",
            "\n",
            "----\n",
            "C: Brian is not a nurse, Catherine is.\n",
            "Q: Who is a nurse?\n",
            "A: Catherine\n",
            "P: Brian\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Temporal\n",
            "\n",
            "There was a change in profession\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Both Scott and Alex were accountants, but there was a change in Scott, who is now an escort.\n",
            "Q: Who is an escort?\n",
            "A: Scott\n",
            "P: Scott and Alex were accountants, but there was a change in Scott\n",
            "\n",
            "C: Both Alex and Scott were accountants, but there was a change in Scott, who is now an escort.\n",
            "Q: Who is an escort?\n",
            "A: Scott\n",
            "P: Scott were accountants, but there was a change in Scott\n",
            "\n",
            "\n",
            "----\n",
            "C: Both Marie and Robin were interpreters, but there was a change in Robin, who is now an agent.\n",
            "Q: Who is an agent?\n",
            "A: Robin\n",
            "P: Robin were interpreters, but there was a change in Robin\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "Understanding before / after -> first / last.\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Carolyn became a waitress before Alexander did.\n",
            "Q: Who became a waitress last?\n",
            "A: Alexander\n",
            "P: Carolyn\n",
            "\n",
            "C: Alexander became a waitress after Carolyn did.\n",
            "Q: Who became a waitress last?\n",
            "A: Alexander\n",
            "P: Carolyn\n",
            "\n",
            "\n",
            "----\n",
            "C: Kathy became a economist before Nick did.\n",
            "Q: Who became a economist last?\n",
            "A: Nick\n",
            "P: Kathy\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Negation\n",
            "\n",
            "Negation in context, may or may not be in question\n",
            "Test cases:      2\n",
            "Fails (rate):    1 (50.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Heather is not an investor. Arthur is.\n",
            "Q: Who is an investor?\n",
            "A: Arthur\n",
            "P: Heather\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "Negation in question only.\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Jane is an analyst. Andrew is an interpreter.\n",
            "Q: Who is not an analyst?\n",
            "A: Andrew\n",
            "P: Jane\n",
            "\n",
            "C: Jane is an analyst. Andrew is an interpreter.\n",
            "Q: Who is not an interpreter?\n",
            "A: Jane\n",
            "P: Andrew\n",
            "\n",
            "C: Andrew is an interpreter. Jane is an analyst.\n",
            "Q: Who is not an analyst?\n",
            "A: Andrew\n",
            "P: Jane\n",
            "\n",
            "\n",
            "----\n",
            "C: Sophie is an economist. Martin is an attorney.\n",
            "Q: Who is not an economist?\n",
            "A: Martin\n",
            "P: Sophie\n",
            "\n",
            "C: Sophie is an economist. Martin is an attorney.\n",
            "Q: Who is not an attorney?\n",
            "A: Sophie\n",
            "P: Martin\n",
            "\n",
            "C: Martin is an attorney. Sophie is an economist.\n",
            "Q: Who is not an economist?\n",
            "A: Martin\n",
            "P: Sophie\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Coref\n",
            "\n",
            "Basic coref, he / she\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Eric and Carolyn are friends. He is a model, and she is a DJ.\n",
            "Q: Who is a model?\n",
            "A: Eric\n",
            "P: Eric and Carolyn\n",
            "\n",
            "C: Carolyn and Eric are friends. He is a model, and she is a DJ.\n",
            "Q: Who is a DJ?\n",
            "A: Carolyn\n",
            "P: Eric\n",
            "\n",
            "C: Eric and Carolyn are friends. She is a DJ, and he is a model.\n",
            "Q: Who is a model?\n",
            "A: Eric\n",
            "P: Carolyn\n",
            "\n",
            "\n",
            "----\n",
            "C: Walter and Annie are friends. He is a secretary, and she is an architect.\n",
            "Q: Who is a secretary?\n",
            "A: Walter\n",
            "P: Walter and Annie\n",
            "\n",
            "C: Annie and Walter are friends. He is a secretary, and she is an architect.\n",
            "Q: Who is an architect?\n",
            "A: Annie\n",
            "P: Walter\n",
            "\n",
            "C: Walter and Annie are friends. She is an architect, and he is a secretary.\n",
            "Q: Who is a secretary?\n",
            "A: Walter\n",
            "P: Annie\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "Basic coref, his / her\n",
            "Test cases:      4\n",
            "Fails (rate):    4 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: John and Carol are friends. His mom is an organizer.\n",
            "Q: Whose mom is an organizer?\n",
            "A: John\n",
            "P: John and Carol\n",
            "\n",
            "\n",
            "----\n",
            "C: Fiona and Louis are friends. Her mom is an editor.\n",
            "Q: Whose mom is an editor?\n",
            "A: Fiona\n",
            "P: Fiona and Louis\n",
            "\n",
            "\n",
            "----\n",
            "C: Richard and Melissa are friends. His mom is an editor.\n",
            "Q: Whose mom is an editor?\n",
            "A: Richard\n",
            "P: Melissa\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "Former / Latter\n",
            "Test cases:      2\n",
            "Fails (rate):    2 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Sophie and Rebecca are friends. The former is an editor.\n",
            "Q: Who is an editor?\n",
            "A: Sophie\n",
            "P: Rebecca\n",
            "\n",
            "C: Sophie and Rebecca are friends. The former is an editor and the latter is an author.\n",
            "Q: Who is an editor?\n",
            "A: Sophie\n",
            "P: Sophie and Rebecca\n",
            "\n",
            "\n",
            "----\n",
            "C: Wendy and Karen are friends. The former is an executive.\n",
            "Q: Who is an executive?\n",
            "A: Wendy\n",
            "P: Karen\n",
            "\n",
            "C: Wendy and Karen are friends. The former is an executive and the latter is an organizer.\n",
            "Q: Who is an executive?\n",
            "A: Wendy\n",
            "P: Karen\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SRL\n",
            "\n",
            "Agent / object distinction with 3 agents\n",
            "Test cases:      1\n",
            "Fails (rate):    1 (100.0%)\n",
            "\n",
            "Example fails:\n",
            "C: Bob p Judy. Judy p Melissa.\n",
            "Q: Who p Judy?\n",
            "A: Bob\n",
            "P: Bob p Judy. Judy p Melissa\n",
            "\n",
            "C: Bob p Judy. Melissa is r by Judy.\n",
            "Q: Who p Melissa?\n",
            "A: Judy\n",
            "P: Bob\n",
            "\n",
            "C: Bob p Judy. Melissa is r by Judy.\n",
            "Q: Who is r by Bob?\n",
            "A: Judy\n",
            "P: Melissa\n",
            "\n",
            "\n",
            "----\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_fair(suite.tests['M/F failure rates should be similar for different professions'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEUk3qGvWZyT",
        "outputId": "f5cff3c8-99fd-48d7-cb72-f47cb499ca53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "profession    fail_men fail_women (count)\n",
            "CEO             0.0      100.0     (4)\n",
            "nurse           40.0      40.0     (5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Error Analysis"
      ],
      "metadata": {
        "id": "zWQBAUV8ZKOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the incorrect predictions\n",
        "incorrect_predictions = []\n",
        "for pred, ref in zip(formatted_predictions, references):\n",
        "    if pred['prediction_text'] not in ref['answers']['text']:\n",
        "        incorrect_predictions.append((pred['id'], pred['prediction_text'], ref['answers']['text']))\n",
        "        \n",
        "# Print the incorrect predictions\n",
        "print(\"Incorrect Predictions:\")\n",
        "for i, (id_, pred, refs) in enumerate(incorrect_predictions[:3]):\n",
        "    print(f\"Example {i+1} - ID: {id_}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "SCvAYvRGC2jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87781aa-3bd2-4167-ea31-86ebd5888cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incorrect Predictions:\n",
            "Example 1 - ID: 56be4db0acb8001400a502ee\n",
            "\n",
            "Example 2 - ID: 56be8e613aeaaa14008c90d1\n",
            "\n",
            "Example 3 - ID: 56bea9923aeaaa14008c91b9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_predictions = []\n",
        "for i, prediction in enumerate(formatted_predictions):\n",
        "    pred_text = prediction['prediction_text'].strip()\n",
        "    example_id = prediction['id']\n",
        "    example = next((x for x in references if x['id'] == example_id), None)\n",
        "    if example is None:\n",
        "        continue\n",
        "    answer_texts = example['answers']['text']\n",
        "    question_text = squad_dataset[\"validation\"][i][\"question\"]\n",
        "    if pred_text not in answer_texts:\n",
        "        correct_text = answer_texts[0]\n",
        "        incorrect_predictions.append({\"question\": question_text, \"incorrect_answer\": pred_text, \"correct_answer\": correct_text})\n",
        "print(\"Number of incorrect predictions:\", len(incorrect_predictions))\n",
        "print()\n",
        "for pred in incorrect_predictions[0:20]:\n",
        "    print(\"Question:\", pred[\"question\"])\n",
        "    print(\"answer:\", pred[\"incorrect_answer\"])\n",
        "    print(\"Correct answer:\", pred[\"correct_answer\"])\n",
        "    print(\"--------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "tEImJ0njC5WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a08a1e7-6f50-4eba-e615-d374aefe2341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of incorrect predictions: 2288\n",
            "\n",
            "Question: Where did Super Bowl 50 take place?\n",
            "answer: Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Correct answer: Santa Clara, California\n",
            "--------------------------------------------------\n",
            "Question: What was the theme of Super Bowl 50?\n",
            "answer: golden anniversary\n",
            "Correct answer: \"golden anniversary\"\n",
            "--------------------------------------------------\n",
            "Question: What was the theme of Super Bowl 50?\n",
            "answer: golden anniversary\n",
            "Correct answer: \"golden anniversary\"\n",
            "--------------------------------------------------\n",
            "Question: Where was Super Bowl 50 held?\n",
            "answer: Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Correct answer: Santa Clara, California.\n",
            "--------------------------------------------------\n",
            "Question: The name of the NFL championship game is?\n",
            "answer: Super Bowl 50\n",
            "Correct answer: Super Bowl\n",
            "--------------------------------------------------\n",
            "Question: What 2015 NFL team one the AFC playoff?\n",
            "answer: Carolina Panthers\n",
            "Correct answer: Denver Broncos\n",
            "--------------------------------------------------\n",
            "Question: Who lost to the Broncos in the AFC Championship?\n",
            "answer: Arizona Cardinals\n",
            "Correct answer: New England Patriots\n",
            "--------------------------------------------------\n",
            "Question: How many times have the Panthers been in the Super Bowl?\n",
            "answer: eight\n",
            "Correct answer: 2\n",
            "--------------------------------------------------\n",
            "Question: Who did Denver beat in the AFC championship?\n",
            "answer: Arizona Cardinals\n",
            "Correct answer: New England Patriots\n",
            "--------------------------------------------------\n",
            "Question: Who did Denver beat in the 2015 AFC Championship game?\n",
            "answer: Arizona Cardinals\n",
            "Correct answer: New England Patriots\n",
            "--------------------------------------------------\n",
            "Question: At which Super Bowl did Beyonce headline the halftime show?\n",
            "answer: Super Bowl XLVIII\n",
            "Correct answer: Super Bowl XLVII\n",
            "--------------------------------------------------\n",
            "Question: What halftime performer previously headlined Super Bowl XLVII?\n",
            "answer: Beyoncé and Bruno Mars\n",
            "Correct answer: Beyoncé\n",
            "--------------------------------------------------\n",
            "Question: What halftime performer previously headlined Super Bowl XLVIII?\n",
            "answer: Beyoncé and Bruno Mars\n",
            "Correct answer: Bruno Mars\n",
            "--------------------------------------------------\n",
            "Question: Who was the main performer at this year's halftime show?\n",
            "answer: Beyoncé and Bruno Mars\n",
            "Correct answer: Coldplay\n",
            "--------------------------------------------------\n",
            "Question: Which Super Bowl halftime show did Beyoncé headline?\n",
            "answer: Super Bowl XLVIII\n",
            "Correct answer: Super Bowl XLVII\n",
            "--------------------------------------------------\n",
            "Question: In early 2012, Goodell said that Super Bowl 50 would be what?\n",
            "answer: an important game for us as a league\"\n",
            "Correct answer: spectacular\n",
            "--------------------------------------------------\n",
            "Question: What was the third city that was considered?\n",
            "answer: New Orleans\n",
            "Correct answer: San Francisco\n",
            "--------------------------------------------------\n",
            "Question: What is the name of the stadium where Super Bowl 50 was played?\n",
            "answer: Mercedes-Benz Superdome\n",
            "Correct answer: Levi's Stadium.\n",
            "--------------------------------------------------\n",
            "Question: In what year was the Super Bowl last held in the Miami/South Florida area?\n",
            "answer: 1985\n",
            "Correct answer: 2010\n",
            "--------------------------------------------------\n",
            "Question: What was the entity that stepped in and caused Miami's Sun Life Stadium to no longer be in the running to host Super Bowl 50?\n",
            "answer: Levi's Stadium\n",
            "Correct answer: Florida legislature\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conducted an error analysis on the TACL-BERT and BERT models, both trained on the SQuAD 1.0 dataset and identified several reasons for incorrect predictions. Some potential reasons include imprecise context understanding, over-specificity or insufficient specificity, difficulties handling ambiguity, misinterpretation of questions, and sensitivity to phrasing and paraphrasing.\n",
        "TACL model:\n",
        "1.\tContext Understanding: The model fails to understand context accurately or comprehensively, leading to incorrect answers. Example: It answered \"reciprocating steam engines\" instead of \"steam turbines\" for 20th-century ship propulsion.\n",
        "2.\tSpecificity Issues: The model generated overly specific or insufficiently specific answers. Example: It answered, \"open loop system\" instead of \"open loop\" and \"hydrogen and helium\" instead of \"helium\" in two different questions.\n",
        "3.\tAmbiguity Handling: The model struggles a bit with ambiguous questions or those with multiple valid answers. Example: It answered \"Richard Trevithick and, separately, Oliver Evans\" instead of \"Oliver Evans\" for the creator of an engine using high-pressure steam in 1801.\n",
        "4.\tMisinterpretation or Misrepresentation: The model misunderstands questions, leading to incorrect predictions. Example: It answered \"edge railed rack and pinion Middleton Railway\" instead of \"Middleton Railway\" for the railroad where Salamanca was used.\n",
        "5.\tImplicit Information Capture: The model struggles to capture implicit information or answer questions requiring deeper context understanding. Example: It answered \"relatively little work is required to drive the pump\" instead of the more specific answer about the Rankine cycle's compression stage.\n",
        "6.\tSensitivity to Phrasing and Paraphrasing: The model could have difficulty with differently phrased or paraphrased questions due to biases or the inability to generalize. Example: It answered \"turbine type steam engines\" instead of \"turbine\" for the steam engine producing most electricity today.\n",
        "BERT Model:\n",
        "2.\tContext understanding: BERT struggles with context, leading to incorrect answers. Example: Answering \"the source of most of the chemical energy\" instead of \"chemical energy\" for oxygen's role in combustion.\n",
        "3.\tIncorrect information extraction: BERT extracts wrong information from the text. Example: Answering \"light sky-blue color\" for the clarity of liquid oxygen, instead of \"clear.\"\n",
        "Areas where both models fail, but TACL performs better:\n",
        "1.\tDisambiguation: TACL outperforms BERT in disambiguating concepts. Example: TACL correctly answers \"methane\" as the primary component of natural gas, while BERT lists multiple components.\n",
        "2.\tAnswer specificity: TACL provides more specific answers than BERT. Example: TACL answers \"to generate ATP through cellular respiration\" for mitochondria's role, while BERT answers \"energy production.\"\n",
        "Both models encounter challenges in handling ambiguity, as demonstrated by their inability to correctly answer a question about the creator of a high-pressure steam engine in 1801. Misinterpretation of questions also led to errors, such as when the model provided a more technical response to a question about the railroad on which Salamanca was used.\n",
        "Despite these limitations, TACL-BERT generally outperforms BERT-base-uncased in areas such as context understanding, information extraction, disambiguation, and specificity. This improvement can be attributed to factors like better context comprehension, more accurate extraction, superior disambiguation capabilities, and the generation of more specific answers.\n",
        "Additional Analysis from the paper: The authors could have investigated the model's performance on questions that require an understanding of implicit information or context. They could have performed a more granular analysis of the questions based on the level of context required, which might have helped identify specific areas where the model needs improvement. The authors could have examined the model's sensitivity to phrasing and paraphrasing by creating multiple versions of the same question with different linguistic structures. Analyzing the model's performance on these variations would help identify any biases in the training data or areas where the model struggles to generalize. They could have analyzed the model's performance across different data splits, including varying levels of question similarity between the training and evaluation sets. This would help identify any overfitting issues and provide insights into the model's ability to generalize to new questions.\n",
        "Despite these limitations, TACL-BERT generally outperforms BERT-base-uncased in areas such as context understanding, information extraction, disambiguation, and specificity. The performance of TACL-BERT, both in terms of failures and improvements over the BERT model, can be attributed to several factors. First, the architecture of TACL-BERT allows for more effective context comprehension, which aids in understanding complex questions and producing more accurate answers. Second, the model's enhanced information extraction capabilities enable it to identify and focus on relevant textual information. Third, the TACL-BERT model exhibits superior disambiguation skills, allowing it to distinguish between similar concepts and determine the most appropriate response. Lastly, the model tends to generate more specific answers, which can be beneficial in certain cases but can also lead to over-specificity issues. These attributes contribute to TACL-BERT's overall performance, with the model outperforming BERT in several areas while still facing challenges in handling ambiguity and certain question types.\n",
        "In conclusion, the errors in TACL-BERT and BERT models can stem from factors such as preprocessing, tokenization, model architecture, training data, and hyperparameter tuning. While both models have limitations in understanding context, extracting correct information, disambiguating concepts, and providing sufficiently specific answers, TACL-BERT typically performs better in these areas. Further investigation and optimization of these models can lead to a more accurate and reliable performance in question-answering tasks.\n"
      ],
      "metadata": {
        "id": "WCFLQWS1bUL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Test on TaCL-Bert vs BERT"
      ],
      "metadata": {
        "id": "zKZrLz0waZc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bkOAGFEIb1f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load TACL model\n",
        "tacl_tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/tacl-bert-base-uncased\")\n",
        "tacl_model = AutoModel.from_pretrained(\"cambridgeltl/tacl-bert-base-uncased\")\n",
        "\n",
        "# Load BERT model\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Choose a sentence to compare\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the sentence for both models\n",
        "tacl_tokens = tacl_tokenizer.tokenize(sentence)\n",
        "bert_tokens = bert_tokenizer.tokenize(sentence)\n",
        "\n",
        "# Get the representations for each token in the sentence for both models\n",
        "with torch.no_grad():\n",
        "    tacl_outputs = tacl_model(torch.tensor([tacl_tokenizer.encode(sentence)]))[0][0]\n",
        "    bert_outputs = bert_model(torch.tensor([bert_tokenizer.encode(sentence)]))[0][0]\n",
        "\n",
        "# Calculate self-similarity for both models\n",
        "def self_similarity(outputs):\n",
        "    cos_sim = torch.nn.CosineSimilarity(dim=-1)\n",
        "    similarity = []\n",
        "    for i in range(outputs.shape[0]):\n",
        "        for j in range(i+1, outputs.shape[0]):\n",
        "            similarity.append(cos_sim(outputs[i], outputs[j]).item())\n",
        "    return np.mean(similarity)\n",
        "\n",
        "tacl_self_sim = self_similarity(tacl_outputs)\n",
        "bert_self_sim = self_similarity(bert_outputs)\n",
        "\n",
        "# Print results\n",
        "print(\"TACL self-similarity:\", tacl_self_sim)\n",
        "print(\"BERT self-similarity:\", bert_self_sim)\n",
        "if tacl_self_sim < bert_self_sim:\n",
        "    print(\"TACL has more discriminative token representations than BERT.\")\n",
        "else:\n",
        "    print(\"BERT has more discriminative token representations than TACL.\")\n"
      ],
      "metadata": {
        "id": "CD0i6uYoDBE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "7c4e8fa0cb9b48d9a83c50813ea93d4c",
            "89a1d04fcdfd45199a5fe46fa5a004a4",
            "529bce4d1b884927800bfa9e97a44124",
            "425dbdc630084dafb640168f9e268407",
            "e2bc31054aa747b5a5d47576d161f346",
            "8d4bb36f5cce4a4aaf9c995725dd747d",
            "f01e8a71f1b047f5809185645f66c30f",
            "241778645b0a4fa68fa3e53356606d13",
            "58e6414334d94ffb8c08953626a4a731",
            "5c3d4c58f17b4ca6bc1d43f15d377dcb",
            "4420898a8c41458190b829a71983ce44",
            "12f6a3c3ae5d467a8cf38ef669cb2ecf",
            "99c0a836884d41769db6964afbc64668",
            "4d659c67787047e0b206ddbe32887ef6",
            "d5df5a316c034436b85152ffbf805c74",
            "f2abaf95e1314c78ade9b384d6f05020",
            "65cc6071c8904104b056e0dae25af63d",
            "7e7a6b9ed93b4d298825642d402fb929",
            "3eebedecbfbe4426b7f5768f13ccd807",
            "06ef7f6fd5f0446fb518adc98fad4660",
            "497498c8ba43472fa35f51e01c280cf6",
            "2d6739adc89b4c0a89ac64ab167d6f3e",
            "ba0de0041d414853b8465e4e4b8fc905",
            "a1518a94974a44d4a6d8b07eb4d9ff8a",
            "bc6f2d55db7f4f9b87bada8058cab7bd",
            "73a0cf0acd194c7185c234b2d127e110",
            "8154988993cb4c11af9ff98687ea9ba5",
            "d84f828738fb435aa620d4160bc7a3c3",
            "2df61cd35a674e0eb3ba38e287b979c4",
            "af93eed287704c08a0580d58caa48dac",
            "506f92826f9a42f99f3f8f9b72b4956c",
            "e5d34e8c90a046a8a3f74ceb84f01fa5",
            "bf20a319300c497495a139b4b2b06a4a",
            "bdd9aa6a4e2e455b91be57bd41d007b4",
            "cb57878f34ac4fc1baa022b0fd9bf2c4",
            "609e90e6c5ab495bacb7b927e6f507bd",
            "e3ea503c520b4a5098b5369d89896bb1",
            "a45cab3734c541f39f1acbcb645af056",
            "f07181a1c1064d16a75a33803f376022",
            "c6863231bc3848e38153df5206738655",
            "85fe9251c80d43ccb6714ac4a248b8e1",
            "28ec6d8d5b0f4ba48809443dad474afc",
            "967c5893ca3249538ccae16666e3bf1d",
            "9d5cf334ce4f43c691506738babee9be",
            "f3c3d3f88cdc467684bf19790059536c",
            "9bdd9b66a79e46d4b15b37ada6792f99",
            "f528fce8a90448929f687fc9e92bcb76",
            "d19837aec5624cf0a747b62611d2d6dd",
            "470c175a706d4cafb19bfeabf01d51e9",
            "c117281236d84b1bbe0ae307458438e8",
            "8e34c59045654d92a091ae13702d8fd5",
            "6c67360254d54ee7ba93cc0275fffce2",
            "e3bdb8e1dc0447a28e16517fae90809e",
            "049765b18ebc41b1a85cb6cb71186fb4",
            "1acfe78fa1d242e98a49ba149967e3b8"
          ]
        },
        "outputId": "cb606368-cd36-40b6-95f4-4b0659a6bb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c4e8fa0cb9b48d9a83c50813ea93d4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12f6a3c3ae5d467a8cf38ef669cb2ecf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba0de0041d414853b8465e4e4b8fc905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdd9aa6a4e2e455b91be57bd41d007b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3c3d3f88cdc467684bf19790059536c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TACL self-similarity: 0.3019088080547976\n",
            "BERT self-similarity: 0.4077328010038896\n",
            "TACL has more discriminative token representations than BERT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the sample unit test, we used the sentence \"The quick brown fox jumps over the lazy dog.\" as input for both the TACL and BERT models. This sentence is commonly used as it contains every letter of the English alphabet, making it a good choice for testing token representations. The test results showed that the TACL self-similarity was 0.3019, while the BERT self-similarity was 0.4077. TACL achieved better self-similarity scores due to its training process, which specifically optimizes the model for isotropic token representation spaces. In other words, TACL encourages more uniform and evenly distributed token embeddings, leading to better generalization and performance on downstream tasks. This is evident in the lower self-similarity score for TACL compared to BERT in the unit test, which suggests that the TACL model captures more nuanced and distinct features for each token."
      ],
      "metadata": {
        "id": "lA8Tq5C6b7Mc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##References\n",
        "\n",
        "\n",
        "*  https://github.com/yxuansu/TaCL/tree/main/english_benchmark \n",
        "*   https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\n",
        "*   https://github.com/marcotcr/checklist/tree/master\n"
      ],
      "metadata": {
        "id": "rhnqMA63e-lC"
      }
    }
  ]
}